{
  "pid": "5e5c52c9418a84a04625e6cc",
  "eid": "67a4113ee92894e7bf181a4f",
  "title": "E179｜DeepSeek技术解析：为何引发英伟达股价下跌？",
  "task_id": "r28pn7exo6jrq5mz",
  "transcription": [
    {
      "time": "00:00:03",
      "text": "欢迎。",
      "speaker": "发言人1"
    },
    {
      "time": "00:00:04",
      "text": "收听硅谷101，我是红军。大家过年好，大家春节过得怎么样？我是整个春节期间都有被deep sick给刷屏。而且它其实不仅仅是在中国户外，它也是现在硅谷还有华尔街讨论的热点。",
      "speaker": "发言人2"
    },
    {
      "time": "00:00:23",
      "text": "Deep sick是在1月26号那一天登上苹果APP store榜的榜首的。从那之后的18天，它的下载量是1600万次。如果我们对比OpenAI发布ChatGPT的同期下载了，它是ChatGPT同期下载量的1.8倍。现在已经成为了全球140个市场里面下载量最多的应用。",
      "speaker": "发言人2"
    },
    {
      "time": "00:00:48",
      "text": "Deep sik引发关注的另外一点，它的出现也带来了美国科技股的全线下跌。在1月27号的那一天，英伟达它的跌幅就接近17%，市值蒸发了5890亿美元。按理说像deep seek这种低成本、高性能，同时还是开源的模型，它的出现应该是带来整个AI创业的繁荣。那市场就应该需要更多的GPU。所以按照这个逻辑推理，英伟达的股价应该是涨而不是跌。但是为何英伟达不涨反跌呢？这期节目我们就会详细的来解释这个问题。除此之外，我们也会深度解析deep sak的核心技术，以及对整个芯片产业还有开源生态的影响。",
      "speaker": "发言人2"
    },
    {
      "time": "00:01:34",
      "text": "在过去的这一周，我们在硅谷做了十个跟deep sick相关的深度采访，我们就不一一在播客里面呈现了。大家感兴趣的话可以去观看硅谷101的视频。我们现在也正在加班加点的制作中，下面就先请收听我们今天的播客。今天我们来聊一聊，最近可以说是大家都在热议的deep sick。跟我在一起的也是大家的老朋友，加州大学戴维斯分校电子与计算机工程系助理教授陈渝北。Hello, 渝北你好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:02:08",
      "text": "你好。",
      "speaker": "发言人3"
    },
    {
      "time": "00:02:09",
      "text": "对，还有一位是inference点AI的创始人与CEO john月。Hello john你好hello。今天其实正好ebay我觉得你可以从模型跟算法的方向来讲，然后john可以跟我们从算力的方向来分析一下，正好是在两位专业的方向上。我觉得首先可不可以先总体上给大家介绍一下，就是为什么这次deep seek一出来，他立刻不管是在股市上还是说在中美之间的这个讨论，他都引发了大家的关注。那渝北你先从技术上给我们简单分析一下，他有哪些比较惊艳的表现。",
      "speaker": "发言人2"
    },
    {
      "time": "00:02:42",
      "text": "我觉得这次的引发这么大的一个讨论的话，首先就是deep sick在所有的这些language model的团队里面，他本身不是一个非常的知名的一个团队。在之前就很多人其实不知道deep sick，他们很低调。第二个是说这次媒体报道出来的一个talking point，我觉得是说他们低成本，然后用基本上用不是最好的算力，然后用很便宜的价格就超越了OpenAI。我认为这个可能对于很多不了解以前的他们的背景的读者，他会造成一定的冲击，对整个的股市也都会造成一定的冲击。然后第三个是它确实这次deep sik做出来的这个模型，在bench one上跟O1达到了一样的差不多的水平，甚至更好。伯仲之间。所以这个可能对大家也有一个冲击，就是说是不是美国的leadership不存在了。我觉得主要的引发这么大的一个讨论是从这么几点来的。我觉得这个报道还是有一定的片面性的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:03:37",
      "text": "对我觉得在展开以前，我非常想follow一下你刚刚说的，就是是不是美国的leadership不存在了。你怎么看这个问题？这可能也是大家最关心的一个问题。",
      "speaker": "发言人2"
    },
    {
      "time": "00:03:47",
      "text": "我其实觉得当然deep sic的技术很好了。但是我实际上感觉就是说在泛化能力上面，我的一个初步的一个感觉，还是说o one的这个模型在一般的任务这样的泛化能力还要更强一点。第二是说我觉得有一点确实是就是说大家在大模型上的这种技术，大家对它理解和技术或者说做的方法，在一定程度上我觉得这个速度下降了。就是大家想的有点相似，做的很多东西在收敛。那当这个时候的话，他其实不是美国的技术领先的一个问题，而是说是整个学界大家的想法，这种创新性，我其实觉得在这一点上是有convergence。这个确实让大家会觉得有这么一点，就是slow down，就是会下降。",
      "speaker": "发言人3"
    },
    {
      "time": "00:04:30",
      "text": "整个的速度你说创新的速度在下降。",
      "speaker": "发言人2"
    },
    {
      "time": "00:04:33",
      "text": "或者说我感觉大家的想法很多的时候是有点像没有那种你说出来我压根儿就没有想到的这种想法，好多时候在这种越来越少了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:04:43",
      "text": "你指的是比如说大家都开始认同强化学习的方式。",
      "speaker": "发言人2"
    },
    {
      "time": "00:04:46",
      "text": "是这一点吗？我自己一点粗浅的理解，这次我反而觉得这个强化学习是被大家过度提到的一个东西。当然一会儿我们可以再说这个细节。但是从deep sick这次的进展，我认为虽然这个强化学习里面占了很大的一个比重，但是在我看来其实是基础模型本身的能力，这个实际上是很强的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:05:06",
      "text": "你是说V3其实已经是一个很强的模型了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:05:09",
      "text": "对我觉得V三是一个本身它的基础能力是不错的，为什么呢？因为在如果你仔细看他的这个文章里面有一个数字，你可以看到在r one zero没有进行强化学习之前，生成100条。它的成功率如果没记错的话，也已经是在10%左右了，这是一个非常显著的一个性能。",
      "speaker": "发言人3"
    },
    {
      "time": "00:05:28",
      "text": "For o是多少？",
      "speaker": "发言人2"
    },
    {
      "time": "00:05:30",
      "text": "Four跟O的话我印象中是最后到了七八十。但是基本的一个概念就是说他在前期ten percent是一个非常高的number。在我看来他用了一个GRPO的方法，本身的这个policy gradient。这个方法在我看来我的理解是这也算是一个比较粗糙的一个模型，就可以认为。后来在网上有我看到有小伙伴说PPO也可以，就是其他的RL的方法也可以。我觉得如果要是这样的话，那其实最主要的这个进展，我认为是在这个基础模型的前期，它已经达到了一个ten percent。我觉得这个是非常好的一个性能了当模型的基础能力达到一定水平之后，你可以通过一个这样类似于search的方法能够自我进行提升。如果你可以找到一个比较方便的一个reward的话，就是这种学习的奖励函数的话，那你就可以实现自我的提升。所以我觉得这个是一个蛮好的一个message。但是我觉得反而强化学习在这里边的地位是次要的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:06:26",
      "text": "所以我总结你的观点，就是你觉得deep sick之所以好，本质上还是因为V3的表现非常惊艳。V3的表现经验其实是他们比如说用MOE的各种方式去让这个基础模型它的整体的性能更好。然后L一只是说它在这个基础模型之上的一次升级。但是你觉得V3比R1跟r one zero更重要。",
      "speaker": "发言人2"
    },
    {
      "time": "00:06:49",
      "text": "我觉得他们都有一些重要的点。V3的里边的重要的点的话，我认为基本上都在和model架构的efficiency上的提升。我觉得在V三里面的话，有两个比较重要的工作。一个是ME。ME以前的话你会发现不同的expert它的load baLance做的不太好。所以当你把它分散到不同的节点上的话，它的load baLance会有问题。所以他们在这一点上做了一个load baLance的优化。同时他在attention的这个layer的话，它要节省这个KV cache。其实也是在提高这个架构的efficiency，就是它的性能。这两点作为它的核心的创新，然后使得它在一个六百多B的一个M创级别的这种大模型上，然后它的基础模型的表现其实已经挺不错的了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:07:35",
      "text": "这次deep sick r one zero的时候，他们其实做的第一件事情就是说我先设计一个非常简单直观的奖励函数，管它叫做role base的这种奖励函数对吧？然后基本上就是我刚才说的，你要保证你回答的这个数学题它要绝对正确，你的回答格式也要绝对正确。他的一个基本想法就是说我就用deep sik v3的方法，每次你问我一个问题的时候，我回答他100条。然后再从这100条里边去寻找那些增强这些回答对的回答的比重。",
      "speaker": "发言人3"
    },
    {
      "time": "00:08:06",
      "text": "实际上它绕过了reinforce learning，我认为就是强化学习里边最难的一个问题，就是稀疏的奖励。就比如说我回答100条我回答一万条它都不对，那么我其实就没有办法去提升了。因为我就根本就没有一个学习的方向，我因为所有的时候都是错的对吧？反而是说如果我做的这个任务已经有一定的成功率了，我会加强这些成功率的这些部分。我觉得这件事情就使他从一个稀疏的奖励变成了一个比较稠密的奖励。同时我也就不再用去搭桥，去建模，去学中间的一些奖励的函数了。所以我感觉这里边是一个它的借助V3的基础的能力有一个挺大的提升。",
      "speaker": "发言人3"
    },
    {
      "time": "00:08:45",
      "text": "同时在r one zero里面告诉我们，如果一个模型的基础能力已经不错了，那么我是有可能通过这个模型自我进行提升的。其实这种思路和model，predictive control和世界模型一些想法，其实是有很多的相似之处的。只不过是我现在在这里解决了一个最简单的一个问题。",
      "speaker": "发言人3"
    },
    {
      "time": "00:09:05",
      "text": "第二个我觉得看似是一个显而易见，但是这次也产生了很大影响力的一个结果，就是说我可以先训一个这样600B的一个大模型，然后我让他用自己发的方式对吧？他因为他可以回答100次，然后用自我bootstrap的方法逐渐提高这个能力。从原来10%可能后面提到百分之七八十。用这样的一个方式，我先学一个大模型，然后我就可以用大模型去教小模型。然后他们后面做了一个非常有意思的实验，就是说在q one上面做了到1.5B一直到三十几B然后这样的模型他们都做了这样的一个distillation蒸馏学习。",
      "speaker": "发言人3"
    },
    {
      "time": "00:09:38",
      "text": "你用大模型学出来的这些reasoning和planning的能力，你可以来教这些小模型提升他们在相关问题上的表现。我感觉是一个相对来讲比较容易想到的一个点。因为其实在所有的自我增强的这种或者说model predictive control，model base RL等等里边面临一个核心问题。就是说如果你的模型不够好，那么我在上面提升的话，我就刚才又说的这些方法。这种通过search搜索的这种方法，其实表现都不会太好。但是你如果用一个大模型，它的搜索能力可以了，对吧？它本身的自己的模型表现好了以后，然后你已经学到了这些能力，你直接交给小模型。",
      "speaker": "发言人3"
    },
    {
      "time": "00:10:17",
      "text": "这个是可以的。所以我听下来觉得deep seek整体上它是一个组合拳。就是它每一步跟他接下来，比如说他从V3到R1 zero到R1，每一步它的方向上都是有一些策略上的可取之处的那你觉得在硅谷的这些公司里面，比如说像OpenAI g或者像cloud点AI包括lama他们有去沿用这样一套模型的整个的方法吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:10:44",
      "text": "我觉得是有的，就是说很多这种想法其实都是在之前的工作里面都有。比如我印象中在deep seek v的这个模型，他们用到的mot head的这种latent attention。之前应该meta有一篇工作专门讲的就是一个multi token的一个layer，其实也有相似的效果，我觉得应该有很大的借鉴。然后reasoning和planning的话，之前也有过很多这个方面的工作了。当然这个process的这个reward，像这种model base方法，我其实恰恰觉得这次deep sik r1 zero他们取这个名字在一定程度上和这个zero有点像。",
      "speaker": "发言人3"
    },
    {
      "time": "00:11:22",
      "text": "你们是什么时候关注到deep sik这家公司的？",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:25",
      "text": "以前他们应该就一直在发一些文章，但是真正特别仔细的关注还是最近的事情了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:11:31",
      "text": "就是之前一直在听V出来开始。",
      "speaker": "发言人2"
    },
    {
      "time": "00:11:34",
      "text": "但之前应该也有一些文章，他们应该一直在发。",
      "speaker": "发言人3"
    },
    {
      "time": "00:11:39",
      "text": "我应该是12月的时候听说的，也是V对V3，但是当时也没有很注意这件事情。因为大家都觉得可能美国还是AI领先很多国内模型。虽然说跑分跑的好，但是大家也不知道到底是怎么跑的分儿，就没有很多心情去关注。而且尤其对我们这个客户，如果不提的话，我们也不会去深入研究这个东西。他这个应该是微软的CEO最近发了一个twitter，发完以后才火起来的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:12:05",
      "text": "他其实火了好几波儿。我是在2024年的年中，就不停有人发给我deep sick的文章。其实当时还有一个硅谷的媒体叫semi analysis，他们就写了一篇文章。就是说v two是他们见过的现在最好的质量最高的开源模型的文章。那个时候大概2024年的7 8月份有一轮。然后V我印象中最开始是像Andrew aci他们几个意见领袖在twitter上说这个模型很棒。然后开始百v three带货的之后，股市的英伟达的价格跌，其实是在r one出来之后的几天。我其实是在想这个市场它是怎么发生的，为什么在这个模型出来差不多之后的一个多月，才引发了股市上的连锁反应。",
      "speaker": "发言人2"
    },
    {
      "time": "00:12:53",
      "text": "我还是感觉这个媒体的报道给大家的一个印象是说，deep seek他用了很少的钱做出这样的大模型。好像说OpenAI你烧了那么多钱，然后他做的跟你一样。对对对。",
      "speaker": "发言人3"
    },
    {
      "time": "00:13:04",
      "text": "第二个印象是说，其实我觉得整个market是emotional的。就是很多人他可能过来会问我这样问题，就说deep sik是不是不用英伟达的最好的芯片？但是我说他背后的资方是患方，对吧？那我们又知道患方实际上是他是算力的一个大佬，这个也可能报道也会对于不了解患方的人来讲的话，对心理上会造成一定的冲击。所以我认为首先训练所花的总共的研发的成本是不低的。第二个事情是我我认为如果没有搞错的话，也确实用到了英伟达最好的一类芯片。",
      "speaker": "发言人3"
    },
    {
      "time": "00:13:39",
      "text": "从这样的两点出发来讲的话，我其实觉得市场是没有必要因此而恐慌，对吧？就是说英伟达的芯片不再被需要了，随便出来一个小的团队就可以花几百万美金来挑战OpenAI了。如果公众是这样的印象，造成这样的恐慌的话，我觉得是没有必要的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:13:55",
      "text": "我其实是想问一下john，因为你是做GPU的，就是你觉得r one出来对英伟达它到底是利好还是利空？为什么它的股价会跌？",
      "speaker": "发言人2"
    },
    {
      "time": "00:14:05",
      "text": "这应该是一把双刃剑，就是有利好也有利空。利好这边就很明显了，就是deep sick出来，其实它是给了人们很多的想象空间。以前很多人都已经放弃做这种AI model什么，现在它其实是属于给了大家很多信心，让更多的初创出来，可以去试探更多的这种application的应用层面的一些possibility。如果有更多人做APP的话，那其实这是英伟大家最希望看到的一个局面，就是AI整个行业被盘活，大家都需要买更多的卡。",
      "speaker": "发言人1"
    },
    {
      "time": "00:14:34",
      "text": "所以这样子的话，其实看起来是对英伟达更好的那更差的这一面就是英伟达的溢价确实是受到了一些冲击。但是这里头可能很多人刚开始是以为它的壁垒被冲倒了，所以一下就跌的特别多。但是其实就我感觉也不是说壁垒被冲倒了，没有那么严重。",
      "speaker": "发言人1"
    },
    {
      "time": "00:14:51",
      "text": "壁垒是什么？就是英伟达它其实是有两个最大的壁垒。一个是它的infinite band芯片互联，然后另一个是哭的，就是它整个一套调用GPU的这个系统，就是它跟AMD这些其他芯片公司其实已经不是在一个层面在竞争的。因为其他的人都是在争，就是我单张卡我的性能怎么样。但是英伟达其实争的是就我互联的这个技术怎么样。然后我的软件调用软件eco system的维持是怎么样的。所以英伟达真正是这两个壁磊，这两个壁垒，deep seek其实都有稍微冲击到它的溢价，但并没有把它的壁垒给冲垮。",
      "speaker": "发言人1"
    },
    {
      "time": "00:15:26",
      "text": "就是怎么冲到他的溢价呢？刚才李伟说他那个MOE做的优化，其实是有一点削弱林伟达互联的这一块儿的一个重要性。他现在其实可以说就是我不同的expert放在不同的卡上，那我之间的这个互联可以做的不是有那么重要。而且有一些dormant的一些expert不用的时候他就休息了。这其实对英伟达的互联这一块的需求是有一点冲击。",
      "speaker": "发言人1"
    },
    {
      "time": "00:15:48",
      "text": "另一块就是对CUDA这一块，它其实是告诉大家，现在有这么一种可能。以前大家都没有觉得它可以绕过哭的。现在还说我们这个团队其实是可以绕过哭的，直接去用PTX然后做一些optimization。当然这不是说以后所有团队都有这个能力去做这件事情。但他至少提供了一种可能性，就是现在你有可能做这个事情了。那有可能做这个事情以后就导致我有可能不需要买英伟达的卡，或者我不需要买英伟达最先进的这个卡，或者我可以用更小一点的英伟达的卡去跑这个模型。",
      "speaker": "发言人1"
    },
    {
      "time": "00:16:19",
      "text": "什么叫做绕过裤带？他是真的绕过裤带了吗？就是我听到一种说法是说，其实他用的不是那个哭的比较高层的API，但是他还是用了裤带比较底层的这些API。",
      "speaker": "发言人2"
    },
    {
      "time": "00:16:31",
      "text": "对，用词不太准确。就是没有完全绕过CUDA的这个eco system，它可以直接去调用CUDA底下的就不是刚才你说那个很高层那个API，它可以直接去调用PTX，就是在这个instruction set上头一层的这个instruction set，然后它在这一层直接做一些优化。但是这个也是挺大的一个工程，它并不是说任何一个小公司都有能力去做这件事情。",
      "speaker": "发言人1"
    },
    {
      "time": "00:16:54",
      "text": "如果deep sick他有这种能力了，业界其他公司会有这种能力吗？就比如说如果大家现在假设我买不到英伟达的GPU，我用AMD的GPU去做。因为AMD跟英伟达就是你刚刚讲的两个核心壁垒，一个是NV link，还有一个是苦的。如果这两个都有某种程度上受到冲击的话，你觉得现在对AMD这样的公司它会是一个利好吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:17:17",
      "text": "我觉得短期来说对AMD是一个利好，因为AMD我记得他最近已经宣布，他把deep seek给弄过去了。但是长期来看也不好说吧，长期来看我觉得可能还是英伟达，因为这毕竟只是dept sick这一个模型。哭的厉害的地方在于它是一个通用的这种GPU调用的一个软件系统，就是你什么软件过来都可以用酷的，但是dept sick这种做法是它只支持dept sik，所以你后头有别的模型，你还要再重新适配一次。我们就是在赌，以后是不是deep seek就真的是gold standard了，deep sick就真的是OpenAI了，所有的初创都在deep seek上建。如果是这样的话，那对MD挺好的，因为他已经移植过去了deep seek。但如果后面不是deep seek，就比如说deep seek其实它的伟大也是在于他对enforcement learning GRPO这些方法的一些改进。后面的更多模型如果都是用这种方法，你有可能来日方长，那不一定是deep seek。它如果再是别的模型的话，那别模型它又要重新适配，那就还是挺麻烦的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:18:13",
      "text": "还不如用酷的。所以你的核心观点是它动摇了英伟达的这两大非常厉害的NV link跟酷的那从GPU的需求上来看。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:24",
      "text": "首先我没觉得他动摇了这两个壁垒，我觉得这两个壁垒还是非常坚挺的壁垒，只是他对这个溢价有一些冲击，就是你可能收不了那么高的价格了，但是也并不代表其他的竞品能突然就进来。",
      "speaker": "发言人1"
    },
    {
      "time": "00:18:35",
      "text": "就是它一个非常漫长的过程。",
      "speaker": "发言人2"
    },
    {
      "time": "00:18:37",
      "text": "就是其他竞品做的跟这两个壁垒不太一样。就是你可以就像我刚说的，你针对这一个模型你可以绕过这个东西。但是没有人做出英伟达哭的那种，就是通用的我全都可以绕过。所以在这个上面其实是没有冲到英伟达的壁垒，只是deep sik提供了一个，就比如有个墙，现在有一个人，大家以前都觉得翻不过这个墙，现在这个人跳过去了。然后他说你看我可以跳过去，你们也有可能跳过来。",
      "speaker": "发言人1"
    },
    {
      "time": "00:19:01",
      "text": "但如果这个人开源了，大家开始用他的模型了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:04",
      "text": "但是不行的，就是只能他跳过去，不是说就是他跳过去了，别人现在也都能跳过去。这就是这件事情他为什么冲击了溢价，但是没有打倒闭的，就这个墙没有变低，但是这个人现在告诉大家，他可以过去了，他说is possible to go over。那你们其他人能不能过来呢？我只是提供了一个精神上的一个鼓励，就说这件事情是有可能的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:19:26",
      "text": "对GPU的需求会减少。因为他们这次训练成本低，所以某种程度上把这个股市的跌也理解成是不是大家需要更少的GPU就可以训练出更好的模型了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:19:37",
      "text": "对，如果训练这一个模型的话，那是这样。但是DPC出来真正的伟大的意义是在于它重新激发了做AI这些人的热情。那这么来看的话，应该是会有更多的公司出来，他们会买更多的芯片。所以这件事情有可能是溢价降低，销售量变大这么一个事情。所以他最后市值是变大还是变小，你就看你中间的这个比例是怎么样。",
      "speaker": "发言人1"
    },
    {
      "time": "00:19:59",
      "text": "你自己的观点。",
      "speaker": "发言人2"
    },
    {
      "time": "00:20:00",
      "text": "这个不好说吧，这个还是得看application，就是2、五年大家能玩出来什么花样。如果之前觉得application出不来的一大阻力是GPU价格的话，那应该英伟达它的市值会涨，就等于现在这个价格变成10分之1甚至更低了，那这个阻力就不在了。但是如果它的阻力是别的的话。",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:22",
      "text": "那就不好说了。所以其实就是越多的AI应用出来，deep sick把这个门槛降得越多。从GPU的需求上来说，整体上是对英伟达更利好的对。",
      "speaker": "发言人2"
    },
    {
      "time": "00:20:32",
      "text": "因为application出来的这些人，他不会自己雇一个团队去把DPC干这些事儿干一遍，他绕过哭的去搞什么PTX。他出来的会是一些很小的这种公司，他只是想要开箱就可以用的这种solution。所以这样子的话它还是英伟达，所以英伟达应该最想看到的局面是更多。",
      "speaker": "发言人1"
    },
    {
      "time": "00:20:52",
      "text": "的AI公司出来，他们需要的是翠模型的GPU，还是更多的推理芯片。",
      "speaker": "发言人2"
    },
    {
      "time": "00:20:58",
      "text": "我其实个人感觉推理芯片后面也会是英伟达，我不觉得这些小公司长期有一些优势，它短期肯定是大家都有优势，如果一直是用deep seek的话，那确实是有一些。但是长期我觉得推理可能也是英伟达。",
      "speaker": "发言人1"
    },
    {
      "time": "00:21:11",
      "text": "训练也是英伟达。为什么推理是英伟达？",
      "speaker": "发言人2"
    },
    {
      "time": "00:21:15",
      "text": "就是因为他还是哭的，然后他还是这个行业的龙头。刚才说那两个壁垒其实也没有动摇。这些asic公司主要的是两个问题，一个是软件支持不够，另一个是硬件其实是没有壁垒的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:21:29",
      "text": "就像我们看一些rock，servers这种公司，他们其实是一个大的wafer上头，然后做这种芯片。当然其实就是在硬件上看来，它只是一个code core和tensor core之间的一个比例。它并不是说我在某项上做到了英伟达做不到的一个东西。你比如说你TPU，TPU它其实100%都是tensor core。但是你说英伟达要想做100% tensor core，他做不了吗？也不是，他只是觉得100%都是tensor core，没有这个市场。所以他如果觉得就是我哪天想做brock这个比例，我觉得英伟达要做那个没有什么难度。它其实硬件上我至少没有看到很强硬的壁垒，大家其实基本有点趋同的趋势。",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:07",
      "text": "然后软件上是它另一个大的问题，就这些asic的这种公司，软件的维护其实做的都不是特别好，就是连做到PTX那一层的维护都不是特别好。就连TPU，TPU用的也是英伟达的PTX对吧？它的instruction set。所以这两个就导致了可能英伟达还是一直占有龙头地位。",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:28",
      "text": "推理芯片对软件的要求也同样高吗？我其实是在想这样一个问题，就是比如说在整个GPU跟训练的这个芯片上，英伟达有绝对的垄断地位。是因为它的整个的软件你是离不开或者很难绕过这一套系统的。但是推理上它方便绕过去吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:22:48",
      "text": "推测对软件要求也很高，你还是要去调用底下的GPU的一些底层的instruction。",
      "speaker": "发言人1"
    },
    {
      "time": "00:22:53",
      "text": "所以像groot他们不是说自己性能很好，但是它的软件侧其实还是没有建起来。",
      "speaker": "发言人2"
    },
    {
      "time": "00:22:59",
      "text": "对软件测比英伟达软件测还是差的非常远。所以你看他们现在做的模式其实越来越重。他刚开始就做个芯片，现在他又做了自己数据中心，然后又做了自己的云。他就等于把这一套word口全部在站着。但他自己跟英伟达比可差得很远，他凭什么能做的更好？",
      "speaker": "发言人1"
    },
    {
      "time": "00:23:17",
      "text": "那你觉得现在市场上有非常值得关注的芯片公司吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:23:21",
      "text": "我觉AMD有一定可能，但是其他的asic我觉得可能还差一些。但是就看AMD来说，应该跟英伟达还是有很长一段距离。",
      "speaker": "发言人1"
    },
    {
      "time": "00:23:32",
      "text": "如果从投资创业这一块来看你觉得有公司在芯片未来的推理测它是能成的吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:23:40",
      "text": "我个人感觉如果要在芯片这块创新，可能更多聚焦在芯片的软件这一块的维护可能会好一些。而不是去在硬件上做一些比如ddr，tensor core，court core这之间比例的一些调配。我觉得这一块儿其实没有什么意义的。你等于只是在帮英伟达当一个大头兵，去看一下就这个比例有没有人要。但是其实你建立不了什么壁垒，但是软件这块其实是有挺大的优化空间。就是你要做出一套比酷的更优的一个软件，可能这块儿会有很大机会，但也不是一件很简单的事情。",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:14",
      "text": "中国的公司有机会吗？华为所有人。",
      "speaker": "发言人2"
    },
    {
      "time": "00:24:17",
      "text": "都有机会。",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:19",
      "text": "对我是在想这样的一个问题，就是因为美国对中国有芯片禁运，那他们其实也更有动力去在底层上去做一些研发，包括在软件侧去做一些研发。因为你刚刚提到了，其实芯片测它的研发硬件上，大家都有一种趋同的趋势。那如果在这样的一种高压的环境下，比如说有一些公司像华为，他们也财大气粗，对不对？他也可以在软件侧去做一些重的投入，然后把这个生态给逼成了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:24:45",
      "text": "华为是有可能的。如果中国的话在硬件上还是需要做出一些进步的。",
      "speaker": "发言人1"
    },
    {
      "time": "00:24:50",
      "text": "就是硬件还是有门槛的对。",
      "speaker": "发言人2"
    },
    {
      "time": "00:24:51",
      "text": "因为中国连生产芯片的这一块都被制裁了。它不是说英伟达一家在制裁他，它是从高速内存到芯片的组装，到telewest packaging，到底下台积电这块生产芯片到ASML卖给他光刻机，它是整条产业链全部在制裁中国。所以中国如果要出芯片，像华为，那它不仅是一个软件的问题，它是整个制造的这一条链全部要搞出来，然后再把软件搞出来。",
      "speaker": "发言人1"
    },
    {
      "time": "00:25:19",
      "text": "懂了，解释的非常清晰。",
      "speaker": "发言人2"
    },
    {
      "time": "00:25:21",
      "text": "这次其实deep sic出来了以后，我其实反而觉得对英伟达这样的公司是一个利好。之前的话大家有一个想法，transformer出来了以后，这七年之内transformer的这个架构它没有变化。如果以后一直是这样的话，对于英伟达来讲是一个非常不利的消息。因为你像CUDA，它最擅长的一点就是说它是通用型，它支持很多不同的架构。所以在模型架构快速改进的时候，这个是对英伟达是最好的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:25:50",
      "text": "假设一旦说我们从某一天开始，所有的模型的架构不再变了，那你就可以想象，那是不是我就可以做asic了，对吧？那我就可以专门优化这个transformer attention的这个mechanism等等的。但是恰恰是比如说像deep sik，他用到了ME，他用到了late的这种mota的tension。这些对layer进行改进，对这种计算进行改进了以后，又回到了一个模型又要改进了，对吧？",
      "speaker": "发言人3"
    },
    {
      "time": "00:26:14",
      "text": "这个对于英伟达来讲是一个利好，可能会对OpenAI来讲不是非常利好。因为像sam他自己也出来说了，如果你能找到比transformer更好的这个架构，那我们很多的investment都会in trouble。我是觉得eventually这个架构还是会变。你像deep sick这次出来以后，他在推这些架构的改变，包括之前从MOE出来以后推这种架构的改变。这可能对于OpenAI这样的公司来讲的话是一个冲击。",
      "speaker": "发言人3"
    },
    {
      "time": "00:26:40",
      "text": "对，因为我注意到其实大家这次说deep seek l one做的特别好的是推理能力。能不能给我们普通不懂技术的听众普及一下什么叫做推理能力？就是为什么推理是很重要的那如果不是推理，它的另一端是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:26:57",
      "text": "推理的话其实就可以认为是我要做一个复杂的任务，我直接无法输出这个的结果。所以我可能要中间有一些草稿纸，可能要做一些计算。依赖这些中间的过程，我可能输出结果，也可能要做更多的计算。所以基本上你就可以认为不管是推理也好，不管是plan也好，还是reasoning也好，都需要刚才我说的这样的一种能力。就是说我现在要解决一个问题，或者说我看到一件事情它发生了，我想知道它为什么会发生，然后你要往前倒。在我的定义里我管这个叫做reasoning，就是反向的去思考。",
      "speaker": "发言人3"
    },
    {
      "time": "00:27:30",
      "text": "可不可以用现实的生活举一些例子。比如说数学是推理，代码是推理吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:27:36",
      "text": "代码的话其实取决于这个任务的复杂性，它是否需要中间部。凡是需要中间部过程，我认为都属于一种推理的过程。",
      "speaker": "发言人3"
    },
    {
      "time": "00:27:45",
      "text": "不需要中间部的任务有哪些？能不能举几个例子方便大家理解。",
      "speaker": "发言人2"
    },
    {
      "time": "00:27:49",
      "text": "咱们就以这种language model agent的一个方式来举个例子对吧？就是比如说今天的天气怎么样，那么这个任务的话就相当于是一个非常直观的一个问题，对吧？就是说我现在想知道天气是什么样，然后这个语言模型它可以直接调用天气的这个API，把天气给我拿过来，这个任务就结束了，它就不算推理。",
      "speaker": "发言人3"
    },
    {
      "time": "00:28:07",
      "text": "对，不算推理。但是以下这个任务就算是推理。比如说我现在在加州，在帕罗拉to我现在要去纽约，能否给我找一个最快的一条路线？你这个时候你会发现，有几种路线可能是坐这几班飞机，这么着可以过来。然后也可以是说我可以先开车到附近的一个机场，然后再去搭乘这样一班飞机，对吧？有不同的这种选择，那它就需要一些中间部调用API也好，自己的一些思考也好在里面。",
      "speaker": "发言人3"
    },
    {
      "time": "00:28:37",
      "text": "所以发展大模型最核心的是发展他们的推理能力，或者说一个大模型我们去判断它的一些核心的能力，只有哪几部分是特别重要的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:28:48",
      "text": "我觉得发展大模型在我看来有几个非常重要的任务，不光是这种推理和planning。为什么大家对推理和对规划的能力非常的重视？因为很多听众可能都听过一个，就是有一篇非常著名的文章是rich sutton写的，叫做the bitter lesson。然后我觉得这个写得非常好，就是他在2019年写的这篇，然后每年都会让所有的同学全部重新读一遍这个文章就是。Do not try to be too smart.",
      "speaker": "发言人3"
    },
    {
      "time": "00:29:17",
      "text": "这里边他提到了两种最真正的能力来自于，一种是学习，一种是搜索。实际上是学习的过程。你大模型之前的，比如说在这种无监督学习也好，它的fan tune on就是后边的监督的funcinpec好。它其实你可以认为规划到学习这个方面。",
      "speaker": "发言人3"
    },
    {
      "time": "00:29:35",
      "text": "现在的说的这种推理和规划的能力，实际上它是一种搜索的能力，所以这两种它都反复出现deep mind。就比如它冲在前面，其实是它很侧重的是后面的搜索的这个能力，对吧？他如果做这种alpha zero什么的，做这些阿尔法go，他做了很多这样的工作，这两者结合起来显然是应该结合的。然后这次deep sik r one他也做的是说是先通过自我的启发是搜索，然后再通过学习用这样一个role base的方法。其实把这两种能力结合起来，我觉得这是很重要的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:30:07",
      "text": "但是在发展大模型的过程之中，我其实觉得还有一个至关重要的点。我认为如果你想通往AGI的话，是不可能绕过去的一个点。不管是说我们做大模型的各种各样的春泥，实际上最终我们希望能让它实现类人的智能。不管说我希望他能够做问答也好，我能够预测未来也好，可以去做coding也好，或者说去做这种推理和规划的能力。其实都是希望能赋予他人这样的智能。但是人有一个非常的强的能力，就是人的学习的效率极其之高。",
      "speaker": "发言人3"
    },
    {
      "time": "00:30:39",
      "text": "有两种估算，我觉得都是有一定道理的。第一种估算是baby LM这里边的一个估算是到人的13岁之前，你所接收的token还是word小于100命令。第二种估算是说假设从人出生开始，每秒钟你可以take in 30个token，大概是十个词。你每天12个小时，20年你最多是十个billion的token。这两个数放在这儿。我们说lama three，它的pre training其实已经到了15个trAiling了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:31:07",
      "text": "所以你看我们的大模型实际上现在一直在follow的一个思路是说我们要scale。就是更大的模型，更多的数据，更多的计算，然后让我的模型更强，这个没有任何问题。但是你会发现它比人所拥有的数据的数量级已经开始大了三个四个数量级。这个时候可能我们还要反问一个问题，为什么人可以如此高效的方式来学习？",
      "speaker": "发言人3"
    },
    {
      "time": "00:31:30",
      "text": "我认为推理和逻辑还有规划的能力，可能是构成这样一个高效性的原因之一。可能还有因果等等的一些其他的原因。然后我认为发展大模型的过程中，如何实现这样的高效也是非常重要的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:31:45",
      "text": "所以现在大模型跟人脑相比，它的学习效率还是低很多。就是至少从数据上来看低。",
      "speaker": "发言人2"
    },
    {
      "time": "00:31:50",
      "text": "好几个数量级的方式低。这个非常像当年的蒸汽机。刚一开始出来，我记得我看过一个数，就是早年有一个估算，说它的这个能量的效率是0.02 percent。那到今天可能是有20个percent对吧？刚好也差了三个数量级。所以我觉得什么时候我们对数据的燃烧的效率可以提高三个数量级，这是当年我跟一样讨论他启发我的一个问题。",
      "speaker": "发言人3"
    },
    {
      "time": "00:32:12",
      "text": "我们有一次在吃午饭的时候聊到这个问题，什么时候我们觉得数据的我们的燃烧效率可以提高三个数量级的时候。可能这种general的intelligence或者human level intelligence就可能更加可能一些了。但是在此之前，我认为只有scaling可能是不够的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:32:26",
      "text": "对我跟大家解释一，在渝北刚刚提到的Young是指Young queen。他的中文名字是杨丽坤。他是渝北的博士后导师，也是图灵奖得主，跟gel free hinton和yo bengel一起被称作是深度学习三巨头。他同时也是meta的首席科学家，待会儿我们会有一部分专门去讨论AGI。那个时候我们可以详细展开聊一下他的思想。",
      "speaker": "发言人2"
    },
    {
      "time": "00:32:51",
      "text": "在此之前我们先把deep seek的这一部分聊完，我们先说一下开源。你们觉得deep seek他选择开源的这条路对行业的生态具体会有哪些的影响？就比如说我知道最近可能在美国的一个论坛reddit上，大家很多已经开始去部署deep sick的模型了。然后其实我很想知道他选了开源以后，这个开源到底是怎么去全部他让他能把模型做得更好的对。",
      "speaker": "发言人2"
    },
    {
      "time": "00:33:17",
      "text": "最近我们其实也部署了一些deep seek的这种模型在我们平台上面。我觉得它开源其实是一件对整个AI行业非常好的一个事情。因为去年下半年以后，大家会感觉有一点失落，因为AI application看起来都起不来。起步的ID有一大原因就是很多人觉得OpenAI其实把所有application的壁垒基本都能打掉了个百分之八九十。大家其实都是比较惶恐的，就是我做一个什么东西，然后明年是不是OpenAI出个什么O4什么，就把这东西全部带了。那我如果做这个东西建立在OpenAI上的话，那就更麻烦对吧？我建立在OpenAI上，它出了一个新的模型，把我的application完全包含进去了。那我在价格上也没法跟他争，我在功能上没法跟他争那这就导致很多人其实就压手，对吧？他就不太敢去做，然后VC也不太敢进来。",
      "speaker": "发言人1"
    },
    {
      "time": "00:34:03",
      "text": "我觉得这次DPT给他开源，其实对整个行业的一个好处就等于就是大家都有了自己的OpenAI。我其实就是作为一个小的application developer，我不再害怕OpenAI出下一版本淹没或者把我淹到，我没法跟他竞争，或者我的产品就干脆再用他的API，然后我就直接就死了。我现在用的是一个开源的做的非常好的一个模型。这样的话我其实有一定的这种continuity，我就有更大的更多的信心去做更多的application。Deep sik如果他在能有能力去超过OpenAI的话，那这个事情我觉得对整个行业就更好了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:34:37",
      "text": "就等于说是有一条恶龙，现在它不存在了，那大家其实发展的就能更好一些。更多人用它，其实它就跟lama的逻辑是一样的，有更多人用，然后有更多反馈，所以它的模型能做的更好。Dept它其实也是这样。如果有更多的application developer，大家都觉得用这个等于自己有拥有了自己的OpenAI，那它收集数据的速度肯定是比其他model都快很多。",
      "speaker": "发言人1"
    },
    {
      "time": "00:35:01",
      "text": "现在我们能看到一个开源的模型，它在整个的性能上已经跟OpenAI的O1，我们说可以说超过或者说接近，但是基本上是同一量级的，对不对？可以预期OpenAI它很快发了O3 mini之后，开源模型可能也会升级，也会有下一个版本再来超过这些闭源模型。就是我是在想当一个开源模型它的性能足够好的时候，OpenAI就是这些闭源模型，它存在的意义是什么？因为大家就直接可以拿到这个开源模型的底座去用了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:35:32",
      "text": "Deep sick的意义在于它的价格降了很多，它是开源的，它跟OpenAI最前沿的这些模型差不多。好，他不是说比OpenAI已经好了，那确实就说他后来这个闭源模型还有什么意义呢？意义就在于它可能还会是领先的一个趋势，就像苹果安卓对吧？苹果其实还是比安卓好的，就是它的leadership and concentrate，他更有可能做出更好的这个产品。但是开源的意义可能就在于它就像安卓一样，就谁都可以用，然后非常便宜。这样它降低了进入行业的门槛，所以它可能才是真正让这个行业蓬勃的一个因素。然后这些闭源的模型，它有可能是一直领先的。闭源如果还不如开源，那可能就没有意义。他应该是有management上面的这个优势，他应该是超过开源。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:17",
      "text": "那现在看起来确实是有一批闭源不如开源的。",
      "speaker": "发言人2"
    },
    {
      "time": "00:36:22",
      "text": "那就自求多福。如果闭源还不如开源，我也不知道这公司在干什么，你还不如免费好。",
      "speaker": "发言人1"
    },
    {
      "time": "00:36:30",
      "text": "我觉得开源的生态是非常重要的。因为我除了在实验室以外，我之前参与一家公司叫ASF，他也做很多的全站的这种AI应用。然后你会发现一件事情是说，很多这种开源的模型你直接是无法使用的。就是产品级的东西你无法直接使用这些开源的模型。但是如果有这样的开源的模型，可能会大大提高你生产出一个这种产品的模型的，大大提高你的效率。",
      "speaker": "发言人3"
    },
    {
      "time": "00:36:55",
      "text": "所以你像deep sik也好，拉曼也好，我觉得这种开源的这种生态对于整个的community来讲是至关重要的一件事情。因为它降低了所有的AI应用准入门槛。其实见到更多的AI的应用，它有更多的触及。这件事情是对于每一个做AI的人是一个非常的利好的一个消息。你其实不希望我们做了大量的training，但实际上real life里面真正能用的AI的application非常少，对吧？第二是它定价定的非常的高，这样的话对于整个的生态是非常不健康的一种状态。",
      "speaker": "发言人3"
    },
    {
      "time": "00:37:28",
      "text": "所以我认为meta在做的这件事情很重要对吧？就是他这个拉玛一直在坚持open source构建，这样让所有的AI的开发者都可以做自己的应用，虽然lama并没有把这个应用直接给你做完，他给你提供了一个foundation。Foundation顾名思义它其实就是一个地板，对吧？你可以在这个地板之上，你可以构建你所想要构建的这种应用。但是他其实把90%的任务给你做好了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:37:51",
      "text": "我认为更好的这样的foundation其实对于整个生态是非常重要的。OpenAI他下大功夫来优化的一些能力的话，它依然会有这样的优势。但是我们也不希望这个市场上只有OpenAI，那对于所有的人来讲可能都是一个不利的一个消息。",
      "speaker": "发言人3"
    },
    {
      "time": "00:38:07",
      "text": "还有一个问题是deep seek他们是怎么把API接口的价格给降下来的。因为我看了一下它的r one官网写的是每百万输入的token缓存，命中的是一块钱，然后缓存未命中的是四块钱，然后每百万输出的token是16块钱。然后o one的价格我整体算了一下，差不多每个档位都是他们的26到27倍之高。他是怎么把这个API的成本给降下来的？",
      "speaker": "发言人2"
    },
    {
      "time": "00:38:37",
      "text": "他等于是从上到下做了整个的一套优化。从PTS这块怎么调用底下的GPU到MOE的架构的low baLance，整个的它都做一套优化。然后我觉得这里面可能最重要的一点就是它可以降低了对芯片的要求。就是你本来可能非得在H100上A100上跑，然后你现在可能可以用稍微低端一些，或者你甚至可以用OK，你可以用国内的那些严格版的H80或者H20这些卡去跑。那这样它其实就已经大幅度的降低了每个token的成本。然后它里头如果再做优化，比如切割GPU versus GPU这方面的东西，它其实可以降下来来很多。而且OpenAI内部其实也说不定，人家早就降下来了，他只是不想降retail的价格，这也不确定。我觉得主要就是这两个，一个是架构上，一个是芯片可以降级了。",
      "speaker": "发言人1"
    },
    {
      "time": "00:39:26",
      "text": "那芯片降级未来会成为全行业一个比较普遍的事情吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:30",
      "text": "我觉得也不会，因为英伟达的老芯片全都停产了，所以市面上其实有限的。就比如你虽然可以说我这个能在V100上跑，但是V100早就停产了，而且每年它要折旧。所以你可能过两年市面上就没有B100，英伟达，它只会产最新的芯片。",
      "speaker": "发言人1"
    },
    {
      "time": "00:39:48",
      "text": "那它的成本还是低的吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:39:51",
      "text": "如果你在新的芯片上做一些优化，比如像我们做的这种切割GPU，那就有可能会变低。因为它这个模型变小了，我们最近跑它那个七的模型，其实就是20个G左右。那我们就拿一张H100把它切了3分之1，然后就跑这个deep seek，那你成本直接就降了3分之1呗。可能我觉得后来会是更多的虚拟化GPU来降低成本。因为如果只是基于老卡和游戏卡的话，首先游戏卡英伟达是black less，你不能用游戏卡去正规的host这些模型。然后你用老卡，就是刚刚说的老卡停产，而且老卡有很多维护这些问题，所以我并不觉得它会成为一个主流的现象。",
      "speaker": "发言人1"
    },
    {
      "time": "00:40:29",
      "text": "所以其实现在你们是提供给大家去做芯片优化，然后来去节省成本的这样的一个工作的那你最近客户应该是暴增，你觉得这个是受益于deep sick，还是说你们一直在做这件事情？",
      "speaker": "发言人2"
    },
    {
      "time": "00:40:42",
      "text": "我们从去年开始就在搞这件事情，我们也是一直在赌后面会有更多的小模型。然后刚好dpc出来以后，deep sik刚才说就是有带来的一个趋势，也是它会蒸馏出更多的小模型。大家如果跑更多小模型的话，其实就需要不同型号的芯片。如果每次都去用物理芯片的话。",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:00",
      "text": "可能是比较难弄的。刚刚其实我们有提到deep sik，他让他的整个的API成本降低了。你刚刚也分析过他的这个研究方法，就是他的这套研究方法，未来你觉得他们有可能会用到更多的。比如说你们在做U的分片跟客户的一些模型中，它的这个研究方法会不会带来整个行业的一次大家对GPU成本更低的一次节省。",
      "speaker": "发言人2"
    },
    {
      "time": "00:41:23",
      "text": "你就说他reinforcement learning的那些方法。对，应该，就他这个出来了，应该是给行业证明了现在有更优的RL的一个方法。我觉得后面肯定会有很多人用相同的方法去做这个事情，而且有其实他自己去调用CUDA这一块儿，以前可能没有人有勇气去试这件事情。当然他们证明了就我们这么几个博士生毕业也可以很快弄一个绕过你们哭的那后面可能很多的这种模型公司都会去效仿。那这样的话应该是大家都这么搞的话。",
      "speaker": "发言人1"
    },
    {
      "time": "00:41:54",
      "text": "成本肯定会下降。所以我理解训练成本降低了，推理成本也大幅的下降了。对，你们现在帮客户去部署这种GPU的时候，客户的主要需求是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:42:04",
      "text": "简单便捷很快的部署上来，价格弹性价格低。",
      "speaker": "发言人1"
    },
    {
      "time": "00:42:09",
      "text": "这个价格低指的是前面部署的，还是说整个后面的一套的解决方案。",
      "speaker": "发言人2"
    },
    {
      "time": "00:42:15",
      "text": "就所有地方价格低他都是开心的，但是我们只能解决他部署这一块的这个成本，其实是有很多浪费的。所以我们在做这个技术，就是你比如说拿一张A100H100，它都是80个G但你要蒸馏出来一些小模型，或者就是你就用现有的什么snowflake，data break的这种模型，那也就是个十个G有的还更小。那其实你在80季上部署一个十个G的东西，你就等于大部分的GPU全部浪费了。但是你还是要付整个GPU的钱。",
      "speaker": "发言人1"
    },
    {
      "time": "00:42:41",
      "text": "假如你用H100，你其实是想用它的那个速度，你想要它那个四纳米那个速度，所以你还是要整张卡租。然后你在influence的时候，你的workload其实是一个弹性的。就是有时候你客户就增了很多，有时候就减少了。如果你一张卡上浪费了很多space的话，你扩的时候你其实每张卡上你都浪费了那么多。那那现在我们在做的这个事情就等于说是我把它虚拟化了以后，你就完全没有浪费。就等于比较简单粗暴的解决了很多GPU部署成本的问题。",
      "speaker": "发言人1"
    },
    {
      "time": "00:43:09",
      "text": "这个领域其实还有一个有意思发现，就是说在过去的6到8个月，我觉得这种小模型的能力进展非常之快。这带来将来一个变革。就是说我之前一开始说到了全世界有99%的算力是对大家不可见的。大家不会觉得一个ARM的芯片里，一个高通的芯片里面，它有这个AI的能力。",
      "speaker": "发言人3"
    },
    {
      "time": "00:43:29",
      "text": "未来的话，如果有大量的这种小语言模型，然后有各种各样的这种VLM，有audio intelligence等等的这些能力。可能会越来越多的出现在曾经不会被用到的这种平台上。现在在特斯拉的车上已经用到了很多。但是越来越多的时候你会发现手机里、耳机里、眼镜里眼镜里这个也是一个火爆的一个单品。现在出来很多眼镜的公司，但越来越多的这些设备里边也会出来这种on device的AI他们对于降低成本，提高AI的可用性，我其实觉得未来是有巨大的机会的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:44:01",
      "text": "小模型好用吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:44:03",
      "text": "小模型其实在很多的领域有很多的基本的应用。你可以发现当你把小模型给到足够的充电以后，它其实最终和大模型的性能其实差不多。",
      "speaker": "发言人3"
    },
    {
      "time": "00:44:13",
      "text": "说一个具体的应用场景。",
      "speaker": "发言人2"
    },
    {
      "time": "00:44:15",
      "text": "就比如说咱们现在正在录制这个节目，咱们用的这个话筒，话筒里面会有降噪的这些功能。然后你这个降噪的功能，你可以做出来极限小的new network。这神经网络它其实可以就放在话筒里边，已经把模型放大十倍，放大100倍，然后你会发现它们的性能差不太多，就是最后的这SNR其实没有太大的变化。那这个时候你就可以把它放到这里边了。以后所有的话筒里面都会跑一个AI的模型，然后它已经把降噪的这个东西做完了。所以越来越多这样的功能会集成进来。",
      "speaker": "发言人3"
    },
    {
      "time": "00:44:44",
      "text": "比如说小语言模型，我们可以放到一个手表上，那它可以做一些基本的问答，调用API完成一些基本的工作。更复杂的一些工作，它其实可以offload到云上面，做这样的一个层次化的一个智能，对吧？但是它其实很多的这种平台上，一个手表它已经能做非常复杂的这种推理了。而且你的手机上像高通的芯片，其实它的这种推理的能力可以达到50t ops，它其实是一个非常大的一个算力，没有比A100差多少。所以很多小模型它其实是可以胜任很多大模型已经在做的事情。然后这个对于降成本，提高AI的触及程度是有很大的帮助的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:45:20",
      "text": "小模型是本地的还是联网的？本地的。所以我理解未来我们整个世界里面可能会有各种各样的小模型。当这个小模型不够用的时候，他再去调动这种大模型，这样就可以极大的节省这一部分的推理成本。",
      "speaker": "发言人2"
    },
    {
      "time": "00:45:35",
      "text": "对我觉得就是未来的AI的info应该是一个层次化的。它最小的可以到端上面，就是在传感器里边去做一些非常普通的问题。在边上也会有更多的AI的功能，然后到云上对吧？端边云我认为它的未来是一个整体。",
      "speaker": "发言人3"
    },
    {
      "time": "00:45:53",
      "text": "之前我说过一个数字，如果你做一个简单的计算的话，你把全世界刚才我说的端上和边上的这个算力你乘一下，你会发现是全世界HPC里面的GPU的算力的100倍。这个是非常可怕的一件事儿，因为它的量太大了。高性能GPU可能是以百万片的级别在出。但是你像手机，像边上的这种端上的，它都可能是以十个billion，就是这种100亿的这种量级，或者手机比如说是亿这种级别，然后到深色的话，它又会带那么一两个摄像机。当它的volume上去以后，它的加起来的算力实际上是超大的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:46:28",
      "text": "芯片够用吗？比如说高通的它它。",
      "speaker": "发言人2"
    },
    {
      "time": "00:46:30",
      "text": "可以做很多很复杂的功能。就是说从小语言模型一直到visual language model，到audio的ASR什么的，很多的功能都胜任了。所以其实对于这些，我管它叫做初级的AI的功能，不管是agented还是perception的对吧？就感知的很多，我认为在这种edge platform和n point身上都是可以完成的。最后，最复杂的一部分任务会交到云上面来。",
      "speaker": "发言人3"
    },
    {
      "time": "00:46:55",
      "text": "然后第二个事情是你会发现全世界其实99%到90%的数据其实也在端和边上。但是现在大部分的情况是u sit or lose t就比如你不可能把camera的所有的这些video全都传上来。所以如果你有AI的function在端和边上，你可能会能够把最有用的数据传上来。这个其实是价值是巨大的。所以其实现在的这些数据都是没有被unlock的。然后在未来的话，当你的AI的触及程度变多了以后，你可以认为初级的AI模型反而是可以充当大模型的一种数据压缩这样的一个角色。",
      "speaker": "发言人3"
    },
    {
      "time": "00:47:27",
      "text": "对，然后现在大家部署的是deep sick的小模型吗？还是拉玛的？",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:32",
      "text": "其实可能都不是。它一个整个生态有有Q问的，然后有lama的，有deep sick现在出来的一些模型也有很多自研的。所以我觉得整个生态里面其实只能说是越来越多的这样的小模型在涌现，而且他们的能力在快速提高。",
      "speaker": "发言人3"
    },
    {
      "time": "00:47:46",
      "text": "选模型看重的关键点是什么？",
      "speaker": "发言人2"
    },
    {
      "time": "00:47:48",
      "text": "首先它必须得快，对吧？得小，这是它的效率的问题。但是除此之外，它必须得足够好，因为没人会为一个小快但不好用的模型买单的。所以就是说你一定要保证他所处理的任务他能够胜任。这个我认为是叫做鲁棒性，就是AI的鲁棒性，这个很重要。我们就说一个话筒降噪你放到这里了，那它必须得能够保证我的一个音质。它不能最后出来的很粗糙，那我是不会用它的，那我可能还是要用后期的处理软件。",
      "speaker": "发言人3"
    },
    {
      "time": "00:48:18",
      "text": "我理解了。所以其实我觉得在应用端的话，大家看的并不是说最前沿的模型是什么，而是说最适合我的模型是什么。就是哪一个模型它能保证比如说我在话筒里面加一个降噪功能，它最后出来的坚固音质跟降噪，它能调整到一个最优水平。然后在这个情况下选成本最低的就可以了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:48:38",
      "text": "是的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:48:39",
      "text": "最后我们再来讨论一下AGI的问题。我看最近antha o pic的CEO跟创始人darrel m他自己在他的文章里面，他也是说了这个人工智能发展的三大动力曲线。第一个就是scaling law，然后这个我们就不解释了。然后第二个大概就是说在人工智能发展的过程中，通过比如说算法的改进，然后芯片上的改进，各种各样的方法。它可以让你的训练的效率跟数量级再去提升一个，比如说每年4倍或者十倍的速度。",
      "speaker": "发言人2"
    },
    {
      "time": "00:49:10",
      "text": "第三个，它就是说整个的训练范式其实也在改变。比如说从2020年到2023年，整个业界他用的方法就是预训练大模型的方式。但是其实在2024年，几乎所有的公司都意识到了，大家要在预训练的这个模型上加入强化学习这个思维链的方式去训练。但是大家在这个步骤上其实花的钱不够多。就比如说以前从10万到100万美元，如果我把这些步骤，比如说拉到1亿美元会怎么样？",
      "speaker": "发言人2"
    },
    {
      "time": "00:49:39",
      "text": "通过这三个方式，他是觉得整个人工智能的发展，按照现在的速度算，它会是指数级的增长。然后它的智能也会指数级的提升。所以它有一个结论非常震惊我，他就说大概在2026年到2027年，他觉得AI会在任何行业，任何场景下比绝大部分的人会聪明。他用的是almost这个词，almost我觉得应该99%。对。",
      "speaker": "发言人2"
    },
    {
      "time": "00:50:07",
      "text": "他做出这样一个预测，就是2026年、2027年这样一个预测，我认为还是非常有勇气的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:50:14",
      "text": "你觉得没有那么快。",
      "speaker": "发言人2"
    },
    {
      "time": "00:50:15",
      "text": "如果你说五年以后的话，我认为可能是更稳妥一点。因为五年以后的事情谁都不知道怎么回事。但是如果你说是明年的事儿，因为现在是2025年了。如果说你是明年在各行各业都要超越大部分的人，或者是我再多给一年2027年，我认为这是有挑战的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:50:32",
      "text": "单个任务比如说写代码。",
      "speaker": "发言人2"
    },
    {
      "time": "00:50:34",
      "text": "写代码其实是提高效率，但是你说超越人我觉得不是这样的。就是很多你会发现这些低级别的任务确实很繁琐，它确实可以加速。但是人工智能它真正用在应用里的开发还远远不够。因为你如果这么去想一下，全世界的算力，它主要是应用在训练上还是应用在推理上？以前可能主要的都是应用在训练上。随着这种大模型的能力变强，随着AI的开发成本下降，所以它这个AI的应用上的所用到的算力其实已经提高了很多了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:51:08",
      "text": "对我觉得替代人可能是他是一个应用层面的事情。但我觉得从他的角度来说，因为他其实自己是一个大模型公司的创始人。我想他说的应该是指模型能达到的这个智力水平。",
      "speaker": "发言人2"
    },
    {
      "time": "00:51:21",
      "text": "如果不解决学习的efficiency的这个问题的话，我其实觉得大模型的智力水平的话是无法跟人真的放在一个级别上的OK。",
      "speaker": "发言人3"
    },
    {
      "time": "00:51:30",
      "text": "所以你觉得达到一样说的这个efficiency，它大概需要多久？就比如说三个数量级。",
      "speaker": "发言人2"
    },
    {
      "time": "00:51:36",
      "text": "对，就是我们当时讨论的时候这个三个摄像机，后来我们就简单我就做了一个这样的一个计算。我其实觉得这个还有非常大的不可预测性，因为这里面需要基础研究。现在我们当前的人工智能跟人所能达到的能力，或者自然natural intelligence和artificial intelligence他们之间的一个efficiency效率差距。我认为不管从功耗，从模型尺寸，从学习所需要的数据都差至少三个数量级。",
      "speaker": "发言人3"
    },
    {
      "time": "00:52:03",
      "text": "跟人相比对功耗的话，你可以想人的这个大脑其实非常efficient。它在运转的时候，你在醒的时候，它的功耗大概相当于一个二十瓦。同样的话，你可以想象我如果要是有一个600个billion prieto这样的一个模型的话，你就可以想象大概我需要多少张卡呢？大概是需要16张卡，这个样子大概对应的是一个2万瓦的一个量级。那么A你看20瓦和2万瓦差了三个数量级。人假设咱们在20岁之前只能access不超过十个billion token，那么lama的这个训练的数据量也是15个trAiling。那么十五锤炼和十个兵链的话，那又是刚好是大概三个数量级这个样子。",
      "speaker": "发言人3"
    },
    {
      "time": "00:52:41",
      "text": "第三个例子，你说如果你看自然界的这些小动物，你看像张斌spider，他可以做非常复杂的三维的navigation这样的模型。在我们现在自动驾驶里面所做的这些navigation的模型的话，基本上是billion级别的prime ter。而你看jumping spider它只有几百万个神经元，刚好又是三个数量级。所以我觉得自然的natural intelligence ts和artificial intelligence ence，它存在这样至少一个三个数量级的efficiency的差距。如何unlock这个差距或者bridge这个gap，我是觉得需要基础研究。",
      "speaker": "发言人3"
    },
    {
      "time": "00:53:11",
      "text": "现在整个大模型的进步让你看到了这个希望吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:53:15",
      "text": "我觉得大模型的进步其实并没有看到这个希望。但是大模型之外，像这种reasoning planning或者是neural symbolic的recommences这种逻辑推理也好，然后像这种search也好，像英国coca ty也好，我觉得这个似乎都在正确的路上。而且这种data的creation，这个data的迭代，就是人其实是非常擅长的做一件事情。就是说我学一个东西，然后我找到哪里学习最有效，然后focus上学习这些东西？我要搭桥等等的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:53:43",
      "text": "原则上来讲的话，如果机器能有这样的能力自我去提升，我认为这个就像人相似了。但是目前很多的时候你的机器的提升是来自于人来给你准备数据。所以就是human in the loop，对吧？然后他要给你做更好的。什么时候这个过程能够基本自动化模型，他们自己给他一个internet，他上去自己提升自己，能够make every token count的话。我认为这样的话可能距离人的智能就更加接近了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:54:10",
      "text": "所以你是觉得整个基础研究已经走在了正确的路上，类似于类人智能的这些基础研究基本上在全面开花。",
      "speaker": "发言人2"
    },
    {
      "time": "00:54:18",
      "text": "这是我觉得是有有很多这样的比较有意思的研究。但是基础研究它虽然我们的发展速度一直在加快，但是基础研究本身有一种不可预测性。有可能明天有个小神童什么的，就找出来一种ILOK的这个密码，然后大家就happy了。这个问题sop掉了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:54:34",
      "text": "它是计算机领域的研究还是人脑领域的研究？",
      "speaker": "发言人2"
    },
    {
      "time": "00:54:37",
      "text": "我觉得他其实是在数学、计算机和人脑工程几个的一个交叉的一个进展。当然目前的话工程占主导地位，就是在工程上面你可能尝试很多这样的想法。",
      "speaker": "发言人3"
    },
    {
      "time": "00:54:50",
      "text": "就是更快。",
      "speaker": "发言人2"
    },
    {
      "time": "00:54:50",
      "text": "主要是工程上的，我认为在这件事情工程上面走的在最近的十年都走的比其他领域都要快一点。因为主要是你可以做实验，不管你怎么想想，你最后可以有更高效的实验效率。因为你数学的话，你要证明的话，其实有的时候这个数据太复杂了，那你无法证明对吧？所以计算机或者说工程in general，它close这个loop更方便一些。",
      "speaker": "发言人3"
    },
    {
      "time": "00:55:13",
      "text": "对，就你对达瑞尔的观点有什么想法吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:55:16",
      "text": "我觉得这说的差不多。我主要觉得他说完全能超过人，这里头最大的一个瓶颈可能在于domain knowledge。比如像我创业这么多年，你要说一个机器能超过我创业的经验，这我觉得可能比较难。因为我创业经验只有我自己知道，对吧？我也没有把这些东西数字化了。在不同的行业里头，大家都会有自己的经验，在增加自己在某项工作中的一些wisdom。他们是不是把他们所有知道的东西全都数据化了呢？而且把这些数字化的这些数据都给到了某一个模型，我觉得这个可能是比较大的一个工程。所以就基于这点，我觉得机器超越人可能没有那么快。我觉应该不会是这两年的事情。主要我觉得就是一个数据的一个壁垒。",
      "speaker": "发言人1"
    },
    {
      "time": "00:56:00",
      "text": "所以你觉得现在数据会是整个大模型训练中遇到的一个达到AGI的一个核心的门槛吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:06",
      "text": "我认为还是模型自我提升能力。",
      "speaker": "发言人3"
    },
    {
      "time": "00:56:09",
      "text": "就还不到数据的能力。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:10",
      "text": "我认为是他如何自己去提升。人在学习的过程中，其实你是不断在寻找学习的信号，然后自己去做一些思考的。很多的这些思考都是内化的，可能不是外界来的。这个token.",
      "speaker": "发言人3"
    },
    {
      "time": "00:56:22",
      "text": "不是说我们外界的整个世界的数据不够了，所以我学不好就学不好。",
      "speaker": "发言人2"
    },
    {
      "time": "00:56:27",
      "text": "还是我自己的问题。然后还有第二点是说人，就是你如果看专家的学习的，它其实是一代人比一代人更强的。第一代人他我要反复尝试，这尝试有点像是一个搜索的一个过程。比如我有一天在野外蹦蹦蹦蹦蹦到一个地方，然后跳出来一个蘑菇，然后我可能会把这个记录下来，它就变成了后人的一个knowledge。所以人其实学习的高效性，很多的时候也不是说因为我探索的高效性。而是说我把之前学习最重要的一些knowledge记下来，然后后面所有的人的学习效率都提高了。不管是围棋也好，音乐也好，每一代人他们探索出来的一些新的技术，它会被记录下来，后面的人就会用上这种高效的学习方法。",
      "speaker": "发言人3"
    },
    {
      "time": "00:57:09",
      "text": "之前有一本写的非常好的书叫做pick，就是讲如何做专注的这种学习。我其实觉得大模型可能来训练之前的大模型，然后用它做出更好的数据，然后再提升自己，然后训练出来下一代的大模型，也可能这会变成一个趋势。但是说到底我认为还是要解决efficiency的问题。就是说我们如何用少量的token获得那么强的泛化能力，我觉得这个是一个圣杯问题之一。",
      "speaker": "发言人3"
    },
    {
      "time": "00:57:34",
      "text": "现在业界有哪些流派，最近其实争议挺多的。我看Young之前就是你的博士生导师杨乐坤，他一直是不太认同纯强化学习的方式的。Anthropic其实他们在solid t3.5之后，它一直还没有特别好的一个推理模型出来。但是比如说OpenAI o one的这个推理模型的思路，包括这次deep sk放出来的r one的这个思路。我觉得其实在业界在2024年可能也是一个稍微标准一点的做法了。当然让他也有不同的看法。所以我是在想在整个大模型的训练的范式的转变上，你觉得业界现在还是一个争议很多大家有完全不一样的思路跟方法，还是慢慢的就像你刚刚说的，你觉得没有创新了，大家都是皈依了。",
      "speaker": "发言人2"
    },
    {
      "time": "00:58:21",
      "text": "我觉得有很多创新。只不过说当你把很多的capital仆人进来以后，你说我要快速scale，大家scaling的方式有点让我看起来感觉好多的时候是有点单调的。创新的点其实还是有很多的，我其实觉得要一直在推动的叫做世界模型。",
      "speaker": "发言人3"
    },
    {
      "time": "00:58:37",
      "text": "这个世界模型其实当时是因为我们看了David had那个文章叫word model。我觉得概念上没什么新的，但是名字很酷，一个polish的名字，而不是说几十年前就有的这个概念。然后我们就后来就adopt这样的一个名字。在我看来这个定义其实超级简单。世界模型其实就做了一件事情，就是说给定当前的状态，给定当前的action，就是你的行为预测未来。当然你能预测的越好就越好吧，但只不过说大家有不同的侧重，这个是一个非常重要的概念。我觉得世界模型是一定应该发展的。",
      "speaker": "发言人3"
    },
    {
      "time": "00:59:10",
      "text": "举个例子，就是你刚刚定义的那三步，只能举一个具举个例子吗？",
      "speaker": "发言人2"
    },
    {
      "time": "00:59:15",
      "text": "什么是世界模型？一个具体例子，其实GPT你可以认为和世界模型相当像，就是generation betrayal。它里边有一个事情就是说给定之前的context，过去的token就所有的词。",
      "speaker": "发言人3"
    },
    {
      "time": "00:59:27",
      "text": "然后预测下一个各种数据。",
      "speaker": "发言人2"
    },
    {
      "time": "00:59:29",
      "text": "对我预测下一个，这里面和世界模型的唯一的区别就是世界模型有个action。Action是什么？Action就是比如向前走、向后走，向左走向右走。有一篇deep mind d非常好的文章叫做Jenny two。它里面就是说给我任何一幅画，我可以变成这个幅画里的主人公，我可以向前走，向后走，然后就可以在这个世界里穿梭了。",
      "speaker": "发言人3"
    },
    {
      "time": "00:59:47",
      "text": "GPT其实可以认为是一个没有action的一个word model。它就做一件事，就是给定之前你给我说的话，我预测未来。那么世界模型更general一点的形式就是给定之前发生的事情，给定我接下来要take的一个行为预测未来世界模型。",
      "speaker": "发言人3"
    },
    {
      "time": "01:00:03",
      "text": "当你的模型非常好的时候，你就可以用它来做reasoning和planning。比如举一个具体的例子，我们最近做的一个工作就是实现了这样的一个事情。就是说如果我可以出一个非常好的一个word model，那么我们接下来可以完全的用这个word model来学习一个task。几乎就是说你不需要再看现实世界了。因为你已经创造了一个现实世界的一个copy了。所以你这个时候可以在一些任务上，你可以搭载上任何的一个强化学习的方法。然后你把现实世界换掉，你就用这个世界模型来做，然后你就可以达到一个很高的performance。",
      "speaker": "发言人3"
    },
    {
      "time": "01:00:38",
      "text": "当你的模型进化到一定程度的时候，已经准确到一定程度，你就具有了这样的能力。就是说我可以在自我的空间里面来提高自己的一些行为。甚至你可能不用强化学，你直接可以用model predict ly control，你直接把它搜出来，你就直接把这一条比较好的轨迹搜出来。",
      "speaker": "发言人3"
    },
    {
      "time": "01:00:55",
      "text": "就是像我刚才说的那个，当你的基础base模型已经好到一定程度的时候，你生成100条已经能找到一些比较好的信号了。然后你可以在对它进行强化以后，让它在这个方面提高了很多了以后直接出来。但假设你的模型已经好到一定程度的时候，你生成这100条里面必然有一条是正确的那你不用再学东西了，你直接可以搜出来了。所以我觉得让他所坚持的就是说这种世界模型，我认为是一个非常重要的方向。只不过这里面也有不同流派，有人可能force在生成，有人希望能学到更高级别的感知，然后有些可能强调空间的intelligence，吧？有不同的这种声音。我其实觉得这样是一个好事情。",
      "speaker": "发言人3"
    },
    {
      "time": "01:01:34",
      "text": "大家还是有很多的不同的方式跟方法去达到。我们说AGI也好。",
      "speaker": "发言人2"
    },
    {
      "time": "01:01:41",
      "text": "我觉得有还是有很丰富的想法的在这一点上。",
      "speaker": "发言人3"
    },
    {
      "time": "01:01:44",
      "text": "对，因为现在关于deep seek他们很多的信息都已经公开出来了，因为他们也是一个开源模型。就是想问一下你们对deep sick有没有什么样的问题？就是你们对这家公司还非常好奇的有问题。",
      "speaker": "发言人2"
    },
    {
      "time": "01:01:57",
      "text": "因为他这个文章里面具体的数据的composition，这个在文章里面并没有写出来，然后以及他训练的很多的细节也没有写出来，只是大概层面上。当然我认为不应该全部都公开出来，对吧？这个不合理。但是如果有更多的细节讲出来，能够让大家更好的去复现，这样的工作可能是更好的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:02:19",
      "text": "就是你希望他的数据能写得更细。",
      "speaker": "发言人2"
    },
    {
      "time": "01:02:22",
      "text": "对，但是这也是一个一是所有的这些frontier的research lab都有这样同样的趋势，就是说到数据这个地方也会非常含糊。",
      "speaker": "发言人3"
    },
    {
      "time": "01:02:28",
      "text": "有些事情就没有办法连OpenAI都不敢写。就是所有的大模型公司问到数据都是他们不敢答的对。",
      "speaker": "发言人2"
    },
    {
      "time": "01:02:35",
      "text": "然后甚至你的连数据是如何baLance的，tuition的这个process是具体怎么做的，这些都是不写的。我可以说不写它具体组成，但是我依然可以写它具体是如何curate等等的。但这些细节好多的时候大家也都不写，所以我其实觉得这些东西反而是最关键的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:02:50",
      "text": "其他的一些你说我用search的方法来做这个reasoning planning，这其实很容易想到。当你的模型足够好的时候，它的bootstrap的方法也可以提高性能，这也很容易想到对吧？小模型无法直接bootstrap，用大模型直接bootstrap出来喂给小模型，这个也很容易想到。所以真正不好想到的这些东西，反而一个是数据的具体的composition然后还有就是说在这种architecture里边的一些底层的一创新。我觉得可能这些东西是最关键的。",
      "speaker": "发言人3"
    },
    {
      "time": "01:03:19",
      "text": "所以整体上来说还是想从deep sick上学到更多的东西。对。",
      "speaker": "发言人2"
    },
    {
      "time": "01:03:23",
      "text": "就我可能没有什么问题，但是我可能比较关注的就是deep sik e这家公司，它能不能持续的push the envelope，它能不能持续的跟OpenAI去挑战他。如果他后面能不断地给我们惊喜，让我们看到大家可能最后做application都是在deep seek上。那这个对整个的芯片info这块的格局确实是有比较大的改变。会是什么改变？就我刚刚说因为deep sig它已经绕过CUDA去适配很多东西了，所以如果它能继续占住这个位置的话，那可能很多其他芯片都有一定的机会，对英伟达本身的这个生态也会有一定的挑战。然后溢价是肯定要打下来。但如果下一个，就比如说拉玛四出来，假如是比deep seek好很多的话，那可能以前又回到scratch或者回到刚开始。",
      "speaker": "发言人1"
    },
    {
      "time": "01:04:08",
      "text": "好的，谢谢渝北，谢谢john。",
      "speaker": "发言人2"
    },
    {
      "time": "01:04:10",
      "text": "好，谢谢。",
      "speaker": "发言人1"
    },
    {
      "time": "01:04:13",
      "text": "当然我们今天的播客有从算力、算法还有整个大模型的演化来去聊deep seek。那它对数据的影响是怎么样的呢？如果大家对听到更多的采访感兴趣，我们也考虑会将一些精华的内容放在硅谷101的视频号上。当然我相信我们的很多听众也是AI领域的专家。如果大家对于deep可以有什么样的想法或者想了解的，欢迎给我们写评论，写留言。这就是我们今天的节目，大家可以通过小宇宙、苹果播客spot fan，还有youtube上搜索硅谷101来关注我们。我是红军，感谢大家的收听，再次祝大家新年快乐。",
      "speaker": "发言人2"
    }
  ],
  "lab_info": {
    "summary": "本期播客由一位未具名主持人带领，聚焦于近期备受瞩目的DeepSik模型及其对AI行业的深远影响。DeepSik在苹果应用商店迅速攀升至顶峰，18天内下载量超过1600万次，超过ChatGPT同期下载量的1.8倍，成为全球140个市场的下载冠军。其在中国、硅谷和华尔街的热议，甚至影响了美国科技股的表现。播客深入探讨了DeepSik的技术核心、市场影响力，以及其对芯片产业和开源生态的潜在冲击。通过十个深度采访揭示了该模型的独特之处，鼓励听众观看相关视频获取更多信息。讨论还涉及了DeepSik如何通过低成本、高性能特性挑战现有AI模型开发和训练方式，对GPU市场尤其是英伟达的影响，以及通过开源模型降低AI应用门槛的潜力。最后，播客转向了AI未来发展的关键问题，强调了高效学习、基础研究和技术创新对推进类人智能实现的重要性，展望了AI领域的创新趋势和未来可能的发展方向。",
    "qa_pairs": [
      {
        "question": "Deep sick为何能引发如此大的关注和讨论？",
        "answer": "Deep sick的出色表现体现在多个方面。首先，它并非来自知名团队，却以低成本、较低算力实现了对OpenAI模型超越的结果，这在业界引起了强烈震撼。其次，Deep sick在 benchmarks 上的表现与OpenAI模型伯仲之间，打破了人们对于美国AI技术领先地位的固有认知。最后，媒体报道中提到的强化学习方法在此次Deep sick进展中并非核心，基础模型本身的强大能力才是关键。",
        "time": "00:02:42"
      },
      {
        "question": "你如何看待“美国AI技术领导地位不存在了”这一说法？",
        "answer": "虽然Deep sick的技术很出色，但在泛化能力上，O1模型可能仍具有更强的优势。同时，我认为当前大模型技术的创新速度有所下降，大家都在追求相似的效果，导致技术上的收敛现象增多。这并非美国技术落后，而是整个学界创新性趋于一致。不过，Deep sick的成功确实挑战了大家对于美国AI技术主导地位的看法。",
        "time": "00:03:47"
      },
      {
        "question": "对于强化学习在Deep sick成功中的作用，你怎么看？",
        "answer": "强化学习在Deep sick中虽占据较大比重，但我个人认为其基础模型的能力更为重要。例如，Deep sick V3在没有进行强化学习前就已达到约10%的成功率，这是一个显著的性能表现。强化学习在此处更多是作为一种辅助提升模型性能的方法，而非主要驱动力。",
        "time": "00:04:46"
      },
      {
        "question": "你认为Deep sick之所以优秀的主要原因是什么？",
        "answer": "Deep sick之所以优秀，本质上是因为V3模型表现出色，通过改进基础模型架构的效率（如ME机制和attention层的优化）以及引入role base奖励函数，绕过了强化学习中稀疏奖励的问题，从而实现了自我提升。此外，利用大模型对小模型进行自我训练和蒸馏学习，也是其成功的重要策略之一。",
        "time": "00:07:35"
      },
      {
        "question": "硅谷的其他公司如OpenAI、Cld和 Lama是否采用了类似Deep sick的这套方法论？",
        "answer": "这些公司确实有借鉴并应用类似的技术和思路，比如之前的工作中提到的类似多token layer的结构和 reasoning-planning相关方法。不过，每家公司都有各自的核心技术和创新点，而Deep sick此次的组合拳策略也给业界带来了新的启示。",
        "time": "00:10:44"
      },
      {
        "question": "市场对于DeepAI是否不再需要英伟达芯片的恐慌是否必要？",
        "answer": "没有必要。首先，训练AI所需的研发成本不低，且DeepAI确实使用了英伟达最好的一类芯片。公众过度担忧英伟达不再被需要是不合理的。",
        "time": "00:13:39"
      },
      {
        "question": "John，从GPU角度看，R one（可能是DeepAI）对英伟达是利好还是利空？",
        "answer": "这是一把双刃剑，既有利好也有利空。利好在于它激发了人们对AI模型的信心，让更多初创公司加入尝试和开发新应用，有利于整个AI行业的活跃和GPU需求增长。而利空则体现在英伟达的部分溢价受到冲击，但壁垒并未被冲垮。",
        "time": "00:14:05"
      },
      {
        "question": "DeepAI是如何冲击英伟达溢价的？",
        "answer": "DeepAI通过MOE优化削弱了英伟达互联技术的重要性，并且展示了部分功能可以通过绕过英伟达的CUDA生态系统直接调用PTX指令集进行优化，这在一定程度上降低了对英伟达高端GPU的需求。",
        "time": "00:14:51"
      },
      {
        "question": "如果DeepAI具备某种能力，其他公司如AMD是否会受益？",
        "answer": "短期内对AMD可能是个利好，因为它已经移植了DeepAI技术。然而长期来看，情况还不明朗，因为DeepAI仅针对特定模型优化，若后续其他模型需要重新适配，AMD的优势可能受限。",
        "time": "00:17:17"
      },
      {
        "question": "DeepAI是否动摇了英伟达的两大核心壁垒（NV link和CUDA）？",
        "answer": "没有动摇，这两个壁垒依然坚挺，只是DeepAI提供了一种绕过的方式，降低了人们对GPU性能的预期，从而影响了英伟达的定价能力，但并未真正打破壁垒。",
        "time": "00:18:24"
      },
      {
        "question": "DeepAI出现是否意味着对GPU需求减少？",
        "answer": "训练成本降低确实可能导致短期内对GPU的需求有所减少，但DeepAI重新激发了AI领域的热情，可能会促使更多公司购买更多芯片，最终结果取决于应用创新和市场需求的变化。",
        "time": "00:19:37"
      },
      {
        "question": "对于推理芯片领域，英伟达的地位是否同样稳固？",
        "answer": "推理芯片领域英伟达仍处于领先地位，尽管硬件上没有绝对壁垒，但其在软件维护上的优势使其难以被超越。ASIC公司虽然在硬件上有一定的灵活性，但在软件支持方面存在不足，因此英伟达有望继续保持领先地位。",
        "time": "00:22:28"
      },
      {
        "question": "市场上是否有值得关注的芯片公司？",
        "answer": "AMD有一定潜力，但其他ASIC公司在硬件创新方面可能还存在差距。在芯片领域的投资和创业，未来或许应该更多关注软件层面的优化和创新，而非仅聚焦在硬件架构的调配。",
        "time": "00:23:40"
      },
      {
        "question": "在当前环境下，中国公司（如华为）有机会在芯片领域取得突破吗？",
        "answer": "华为等中国公司确实有机会通过在硬件和软件上的重投入来推动技术和生态的发展，尤其是在美国实施芯片禁运的压力下，中国公司更有动力进行底层研发和生态建设。",
        "time": "00:24:19"
      },
      {
        "question": "这次美国对中国的芯片产业进行了全面制裁，具体涉及哪些环节？",
        "answer": "这次制裁涵盖了从高速内存、芯片组装，到封装测试，再到台积电生产芯片，以及ASML销售光刻机等整个产业链条，几乎涉及所有与中国芯片制造相关的环节。",
        "time": "00:24:51"
      },
      {
        "question": "Deep Learning的出现对英伟达这样的公司是好消息还是坏消息？",
        "answer": "Deep Learning的出现对英伟达可能是一个利好。因为之前大家认为基于transformer架构的模型在未来七年可能不会有太大变化，这对英伟达不利，因其CUDA技术擅长于支持多种架构。然而，Deep Learning等技术通过改进模型架构和计算方法，推动了架构的快速迭代，这反而有利于英伟达的竞争优势。",
        "time": "00:25:21"
      },
      {
        "question": "推理能力在AI中为何重要？",
        "answer": "推理能力是指在解决复杂任务时，模型需要进行中间计算、思考和推导，而非直接输出结果。比如规划路径、做决策等都需要推理能力。它对于AI而言至关重要，因为无论是plan（计划）还是reasoning（推理），都是解决一个问题或理解事物发生原因的基础过程。",
        "time": "00:26:57"
      },
      {
        "question": "代码编程是否属于推理范畴？",
        "answer": "编程是否属于推理取决于任务的复杂性，如果编程过程涉及到中间步骤和思考，那么它就属于推理范畴；如果只需要调用API获取信息，则不算推理。",
        "time": "00:27:36"
      },
      {
        "question": "发展大模型的核心是什么？",
        "answer": "发展大模型的核心包括提升推理能力、规划能力以及学习和搜索能力。其中，推理和规划能力尤其关键，它们与学习相结合，构成了大模型高效处理任务的基础。同时，如何提高学习效率也是通往实现类人智能的重要一步。",
        "time": "00:30:07"
      },
      {
        "question": "Deep Seek选择开源对其行业生态有何影响？",
        "answer": "Deep Seek选择开源能够降低行业门槛，让更多的开发者有信心去开发应用，不再担心因使用闭源模型而被OpenAI的新版本取代。开源模型性能接近甚至在某些方面超过闭源模型，能够促进更多反馈和改进，从而推动整个行业蓬勃发展。对于Deep Seek来说，开源有助于其扩大影响力并可能形成领先趋势，就像苹果和安卓操作系统之间的竞争关系一样。",
        "time": "00:35:01"
      },
      {
        "question": "开源生态对于AI community的重要性是什么？",
        "answer": "开源生态对于整个AI community至关重要，因为它降低了AI应用的准入门槛，让更多的人能够接触并使用AI，而不是只有少数人能进行大量的training后开发实际可用的AI应用。此外，高价的AI应用也不利于生态健康发展。",
        "time": "00:36:55"
      },
      {
        "question": "Meta在构建生态中的作用是什么？",
        "answer": "Meta通过坚持open source构建，为所有AI开发者提供了基础（foundation），即地板，开发者可以在上面构建自己想要的应用，而Meta完成了90%的任务，降低了开发门槛。",
        "time": "00:37:28"
      },
      {
        "question": "是否认为更好的foundation对于AI生态是必要的？",
        "answer": "是的，我认为更好的foundation对整个AI生态非常关键，OpenAI等公司投入大量精力优化的能力可以让他们保持优势，但市场不应仅由OpenAI主导，这对整个行业和生态都不利。",
        "time": "00:37:51"
      },
      {
        "question": "Deep Seek是如何降低API接口价格的？",
        "answer": "Deep Seek从上到下进行了一系列优化，包括PTS调用底层GPU、MOE架构的low balance设计等，并且可能最重要的是降低了对芯片的要求，可以使用稍微低端或国内版本的H80、H20等卡来运行，大幅度降低了每个token的成本。",
        "time": "00:38:37"
      },
      {
        "question": "芯片降级在未来是否会成为全行业的普遍现象？",
        "answer": "不会，因为英伟达的老芯片停产，市面上老卡有限且折旧问题严重。但如果在新芯片上进行优化，比如切割GPU，可能会降低成本。虚拟化GPU也可能成为降低成本的方式之一。",
        "time": "00:39:30"
      },
      {
        "question": "“价格低”具体是指哪些方面的成本？",
        "answer": "“价格低”涵盖所有方面的成本，包括部署成本，但目前存在浪费现象，通过技术手段可以有效解决GPU部署成本问题。",
        "time": "00:41:54"
      },
      {
        "question": "目前你们为客户提供的芯片优化服务需求量增长了吗？这是否受益于Deep Seek？",
        "answer": "我们从去年就开始做芯片优化工作，客户增长确实受到了Deep Seek的影响，因为Deep Seek促使小模型的发展趋势成为可能，进而需要不同型号的芯片。",
        "time": "00:40:42"
      },
      {
        "question": "对于小模型的应用前景有何看法？",
        "answer": "小模型在很多领域有广泛应用，并且随着小模型能力的快速提升，将会带来行业变革，越来越多的小模型会集成在各类设备中，如手机、耳机、眼镜等，有助于降低成本并提高AI的可用性。",
        "time": "00:43:29"
      },
      {
        "question": "是否认为Deep Seek的研究方法会对整个行业带来GPU成本降低的影响？",
        "answer": "是的，Deep Seek提出的强化学习方法证明了更优的可能性，后续肯定会有更多公司效仿，从而带动整个行业GPU成本的下降。",
        "time": "00:41:23"
      },
      {
        "question": "小模型是本地运行还是联网运行？",
        "answer": "小模型通常在本地运行，未来可能会有各种各样的小模型在各种设备上运行，当小模型无法满足需求时，会调动大模型进行复杂任务处理，以此节省推理成本。",
        "time": "00:45:20"
      },
      {
        "question": "在选择模型时看重的关键点是什么？",
        "answer": "首先看重的是模型的速度和小体积以保证效率，其次最重要的是模型必须足够好，具有鲁棒性，能够胜任所需处理的任务。",
        "time": "00:47:48"
      },
      {
        "question": "AGI的发展预测中，人工智能会在何时超越大部分人的智能？",
        "answer": "根据某CEO和创始人darrel m的观点，预测在2026年到2027年，AI将在任何行业和场景下，以99%的概率超越大部分人的智能。",
        "time": "00:49:39"
      },
      {
        "question": "你是否认为2026年到2027年AI就能在所有领域超越人类？",
        "answer": "我觉得这个预测是有勇气的，但如果是明年或后年的事情，我认为实现超越大部分人的智能是有挑战的。",
        "time": "00:50:15"
      },
      {
        "question": "对于大模型在应用层面取代人的可能性怎么看？",
        "answer": "大模型在提高开发效率方面有应用，但在真正应用于实际应用开发中，目前还远远不够，尤其是在算力使用方面，大部分算力仍主要用于训练上。如果要真正取代人，在替代性上还需要很大的提高。",
        "time": "00:50:34"
      },
      {
        "question": "要达到与人类相似的学习效率（即同样的智力水平），需要多久？",
        "answer": "这是一个难以预测的问题，需要基础研究来解决，目前的人工智能与自然智能之间的差距至少有三个数量级，包括功耗、模型尺寸和学习所需数据等方面。",
        "time": "00:51:36"
      },
      {
        "question": "大模型的进步是否让看到了希望？",
        "answer": "大模型的进步并没有明显看到希望，但在推理、逻辑推理、搜索等方面有进展。如果机器能自我提升并优化数据利用效率，会更接近人类智能。",
        "time": "00:53:43"
      },
      {
        "question": "基础研究进展如何，尤其是在类人智能方面？",
        "answer": "尽管有很多有意思的研究，基础研究的发展速度在加快，但仍具有不可预测性。同时，工程上的进步较快，尤其是在实验效率上，但基础研究的本质需要更多探索。",
        "time": "00:54:18"
      },
      {
        "question": "数据是否是大模型训练和达到AGI的一个核心门槛？",
        "answer": "目前看来，模型自我提升能力可能是比数据更重要的挑战，如何用少量token获得强泛化能力是一个圣杯问题之一。",
        "time": "00:57:09"
      },
      {
        "question": "业界在大模型训练范式转变上是否存在争议和不同思路？",
        "answer": "业界确实存在争议和不同思路，如对于纯强化学习方式的看法不一，但同时也有很多创新，例如世界模型的发展，它对于提升模型性能和实现reasoning和planning具有重要作用。",
        "time": "00:57:34"
      },
      {
        "question": "对于Deep Sack模型的具体细节和数据来源是否了解清楚？",
        "answer": "希望Deep Sack能提供更详细的模型数据和训练细节，以便大家能够更好地复现和学习。同时，也期待他们能持续推动创新，挑战OpenAI，从而改变芯片领域格局。",
        "time": "01:01:57"
      },
      {
        "question": "客户在部署GPU时的主要需求是什么？",
        "answer": "客户主要需求是简单便捷快速部署，价格弹性低，包括部署成本和整体解决方案的成本。",
        "time": "00:42:04"
      }
    ],
    "chapters": [
      {
        "time": "00:00:00",
        "title": "深度解析DeepSick：技术影响与市场反应",
        "summary": "本期节目深入探讨了DeepSick的崛起及其对全球科技市场的影响。DeepSick自1月26日登顶苹果App Store榜首后，在18天内下载量达到1600万次，超越同期ChatGPT的下载量，并在140个市场中成为下载量最多的应用。其出现不仅引发了美国科技股的全线下跌，英伟达市值也蒸发了5890亿美元。尽管DeepSick作为低成本、高性能的开源模型应推动AI创业繁荣和对GPU的需求，英伟达股价却反常下跌。节目将详细解释这一现象，并深入解析DeepSick的核心技术以及其对芯片产业和开源生态的影响。此外，通过在硅谷进行的十个深度采访，进一步探讨了DeepSick的多方面影响。"
      },
      {
        "time": "00:02:09",
        "title": "DeepSeek引发的AI技术与算力讨论",
        "summary": "DeepSeek在语言模型领域以其低成本、高效率的表现引起了广泛关注，特别是其在性能上与OpenAI相媲美甚至超越。尽管如此，讨论指出DeepSeek的泛化能力可能略逊于OpenAI的模型。此外，讨论还涉及了当前AI技术领域的创新速度放缓现象，以及强化学习在大模型训练中的角色和有效性。特别强调，基础模型的能力提升是实现自我提升的关键，而强化学习方法在其中的地位可能较为次要。"
      },
      {
        "time": "00:06:25",
        "title": "Deep Sick V3与R10在模型效率与性能提升的探讨",
        "summary": "总结指出，Deep Sick V3之所以表现出色，主要归功于其在模型架构效率上的显著提升，特别是在负载平衡优化和节省KV缓存方面的创新。这些改进使得V3在大规模模型上的基础性能十分优秀。进一步地，R10通过设计直观的奖励函数，如基于角色的奖励机制，解决了强化学习中稀疏奖励的问题，从而实现了模型自我提升的能力。这一策略将稀疏奖励转化为更稠密的奖励，表明当模型的基础能力足够强时，通过自我提升可以进一步增强其性能，这一思路与预测控制和世界模型的概念有诸多相似之处。"
      },
      {
        "time": "00:09:04",
        "title": "利用大模型训练小模型的蒸馏学习方法",
        "summary": "讨论了一种通过先训练大模型，然后利用大模型的能力来指导和优化小模型的蒸馏学习方法。这种方法通过自我增强和搜索能力的提升，逐步提高模型的表现，特别是在reasoning和planning任务上。提到了在不同规模的模型上进行的实验，以及这种方法在硅谷公司如OpenAI和Cloud AI中的潜在应用。"
      },
      {
        "time": "00:10:43",
        "title": "DeepSick公司及其大模型的市场影响",
        "summary": "对话围绕DeepSick公司的最新模型及其在AI领域的成就展开，特别是其模型与OpenAI的比较。讨论中提到了DeepSick的模型在使用较少资源的情况下，达到了与OpenAI相似的性能水平，这引发了市场对DeepSick的关注和对英伟达股价的影响。此外，还分析了市场对DeepSick使用算力和资金的误解，指出DeepSick实际上使用了英伟达的顶级芯片，并且研发成本并不低，强调了市场不应因此对英伟达产生不必要的恐慌。"
      },
      {
        "time": "00:13:54",
        "title": "英伟达GPU技术在AI领域的机遇与挑战",
        "summary": "对话讨论了R One的出现对英伟达的影响，认为这是一把双刃剑，既有利好也有利空。利好方面，R One的出现给AI模型开发带来了新的想象空间，激发了更多初创公司的参与，有助于整个AI行业的活跃，进而增加GPU需求。然而，R One也对英伟达的溢价和壁垒产生了冲击，特别是在芯片互联技术和CUDA调用系统方面。R One通过优化MOE和使用PTX进行优化，减少了对英伟达互联技术和CUDA的依赖，提供了一种可能绕过CUDA生态系统的途径，使得部分应用可能不再需要最先进的英伟达GPU或可以使用较小的GPU运行模型。尽管如此，英伟达的壁垒并未被完全冲垮，其技术优势依然显著。"
      },
      {
        "time": "00:16:53",
        "title": "Deep Sick模型对GPU市场的影响与未来展望",
        "summary": "对话围绕Deep Sick模型对GPU市场，特别是对英伟达（NVIDIA）与AMD的影响进行了深入讨论。尽管Deep Sick可能在某种程度上削弱了英伟达的NV Link和CUDA等核心壁垒的溢价，但这并不意味着这些壁垒被完全动摇。相反，这可能激发了市场对GPU的更多需求，因为Deep Sick展示了训练成本降低的可能性，同时也重新点燃了AI领域从业者的热情。因此，尽管GPU的单价可能会降低，但总体需求和销量可能会上升，最终影响公司市值的关键在于销售量增长与价格下降之间的平衡。"
      },
      {
        "time": "00:19:59",
        "title": "英伟达在AI芯片市场的优势与挑战",
        "summary": "对话围绕英伟达在人工智能芯片市场中的地位及其面临的挑战展开。讨论指出，随着AI应用的增加，对GPU的需求上涨，这整体上利好英伟达。尽管ASIC（专用集成电路）公司试图挑战英伟达的市场地位，但它们面临软件支持不足和硬件壁垒不强的问题。英伟达因其在软件和硬件上的优势，特别是在GPU和推理芯片领域，预计将持续保持其龙头地位。"
      },
      {
        "time": "00:22:28",
        "title": "推理芯片的软件需求及市场竞争",
        "summary": "讨论集中在推理芯片对软件要求是否同样高，以及在GPU和训练芯片领域，英伟达为何能保持垄断地位，主要因其软件系统难以绕过。相比之下，推理芯片虽在硬件性能上可能有优势，但在软件生态建设上仍落后于英伟达，导致一些公司不得不扩展业务模式，包括建立自己的数据中心和云服务，以求全面布局。然而，这些公司在与英伟达竞争时仍面临巨大挑战。"
      },
      {
        "time": "00:23:17",
        "title": "芯片公司与技术创新的市场前景",
        "summary": "对话讨论了芯片公司，特别是AMD和英伟达在市场上的位置，以及中国公司在芯片创新，特别是软件维护方面的潜力。提到了美国对中国的芯片禁运可能推动中国在底层研发和软件侧的投入，同时强调了硬件制造的挑战。此外，还探讨了模型架构的快速变化如何对英伟达等公司产生利好，以及模型架构改进对OpenAI等公司可能带来的冲击。"
      },
      {
        "time": "00:26:39",
        "title": "大模型的推理能力及其对高效学习的影响",
        "summary": "对话围绕大模型的推理能力进行了深入探讨，解释了推理能力在复杂任务中的重要性，以及与规划和学习能力的关联。提到了Richard Sutton的文章《the bitter lesson》，强调了学习和搜索作为核心能力的重要性。讨论还涉及了深度学习模型在高效学习方面与人类的差距，以及如何通过提高数据使用效率来接近人类水平的智能。最后，提及了杨丽坤（Young Queen）及其在深度学习领域的贡献，预示了后续关于AGI的讨论。"
      },
      {
        "time": "00:32:50",
        "title": "Deep Seek开源对AI行业的影响与生态构建",
        "summary": "对话围绕Deep Seek选择开源路径对AI行业生态的影响展开。开源使得更多开发者能部署和改进Deep Seek的模型，从而加速模型的优化和迭代。这种做法降低了AI应用的门槛，激发了行业创新，避免了OpenAI等大型模型服务商可能带来的垄断效应。同时，开源模型的广泛应用和反馈有助于模型性能的提升，形成良性循环。此外，讨论也触及了闭源模型与开源模型的竞争态势，指出闭源模型需保持技术领先性以维持其市场价值。总体而言，Deep Seek的开源策略被视为对整个AI行业生态的积极贡献，促进了AI应用的多元化和普及。"
      },
      {
        "time": "00:38:06",
        "title": "DeepSeek API成本降低及GPU优化策略",
        "summary": "讨论集中于DeepSeek如何通过架构优化和降低芯片需求来大幅降低API接口的价格，使其成本相比竞争对手降低了26到27倍。通过使用更低端的GPU，如国内的H80或H20，DeepSeek成功减少了每个token的成本。此外，通过GPU虚拟化和优化，如切割GPU，进一步降低了成本。尽管老款GPU的停产限制了使用低端GPU的可行性，但虚拟化GPU技术被视为未来降低成本的关键。客户的主要需求是快速、便捷且价格弹性低的GPU部署方案，以避免资源浪费。DeepSeek的研究方法和优化策略预计会推动行业在GPU成本节省方面的进步，尤其是通过更优化的强化学习方法和CUDA调用。"
      },
      {
        "time": "00:43:09",
        "title": "小模型在AI技术发展中的潜力与应用",
        "summary": "在过去6到8个月，小模型在AI领域的进展迅速，预示着未来AI能力将在更多平台上显现，包括手机、耳机和眼镜等。小模型在足够训练后，其性能与大模型相当，这使得它们可以被集成在话筒降噪、手表问答等功能中，实现本地AI处理，从而大幅降低成本并提高AI的普及度。此外，高通等芯片的推理能力强大，足以支持小模型完成复杂任务。未来AI将形成层次化结构，包括端侧、边缘侧和云端，其中大部分数据处理将在端和边缘完成，只有最复杂任务交由云端处理。这种层级化智能有助于更有效地利用和传输数据，同时小模型的高效性和鲁棒性成为选择模型的关键考量因素。"
      },
      {
        "time": "00:48:38",
        "title": "人工智能发展的三大动力与AGI的未来",
        "summary": "讨论集中于人工智能发展的三大动力：scaling law、算法与芯片改进、以及训练范式的变革，特别是预训练大模型与强化学习的结合。CEO与创始人Darrel M预测，到2026年到2027年，AI将在几乎所有行业和场景中超越绝大部分人类的智力，这一观点基于人工智能的指数级增长。然而，对于这一预测的实现速度存在质疑，认为尽管AI在某些低级别任务上能显著提高效率，但在应用层面的开发仍显不足。此外，讨论指出，目前的人工智能与人类自然智能之间存在至少三个数量级的效率差距，尤其是在功耗、模型尺寸和所需数据量方面。为了弥补这一差距，基础研究在数学、计算机和人脑工程的交叉领域显得至关重要，尤其需要探索如何使机器能够自我提升，以更接近人类智能。"
      },
      {
        "time": "00:55:12",
        "title": "机器超越人类的瓶颈与学习效率",
        "summary": "讨论集中在机器是否能够超越人类，特别是指出在特定领域的专业知识（domain knowledge）可能是机器面临的最大瓶颈。讨论者认为，由于个人经验难以数字化，机器难以超越人类在某些领域累积的经验和智慧。此外，提到了数据壁垒和模型自我提升能力是实现AGI（通用人工智能）的关键挑战，强调了学习效率的重要性，包括通过前代人的知识积累提升后代人的学习效率，以及通过少量的token获得强泛化能力的可能性。"
      },
      {
        "time": "00:57:33",
        "title": "大模型训练范式转变与世界模型的未来",
        "summary": "对话讨论了当前人工智能领域中流派与争议，特别是关于强化学习和世界模型的发展。提到了Young和杨乐坤博士对纯强化学习的不同看法，以及Anthropic、OpenAI和DeepMind在推理模型方面的进展。重点阐述了世界模型的概念及其在预测未来状态中的应用，指出其与GPT模型的区别在于引入了action的概念。此外，讨论还涉及了世界模型在reasoning和planning中的潜在作用，以及不同公司在大模型训练中的数据处理和架构创新的重要性。最后，讨论了DeepSeek在推动技术边界和影响芯片市场格局的潜力。"
      }
    ],
    "mindmap": {
      "children": [
        {
          "children": [
            {
              "children": [],
              "content": "登顶苹果App Store榜首，18天下载量达1600万次，超过ChatGPT同期下载量1.8倍"
            },
            {
              "children": [],
              "content": "成为全球140个市场下载量最多的应用"
            },
            {
              "children": [],
              "content": "引发美国科技股全线下跌，英伟达跌幅接近17%，市值蒸发5890亿美元"
            },
            {
              "children": [],
              "content": "低成本、高性能、开源模型，超越OpenAI，引发市场关注"
            },
            {
              "children": [],
              "content": "核心技术解析：V3模型的高效架构优化（ME和attention layer KV cache节省），R1 Zero的强化学习方法（PPO），自我增强与小模型蒸馏学习"
            }
          ],
          "content": "Deep Seek的崛起与技术亮点"
        },
        {
          "children": [
            {
              "children": [],
              "content": "开源生态的推动：降低AI应用门槛，激发更多创新应用，形成竞争与合作的良性循环"
            },
            {
              "children": [],
              "content": "对英伟达的影响：挑战其溢价能力，但未动摇其技术壁垒（NVLink和CUDA生态）"
            },
            {
              "children": [],
              "content": "GPU需求变化：训练成本降低，推理成本大幅下降，虚拟化GPU技术有望降低成本"
            },
            {
              "children": [],
              "content": "小模型的应用：端边云层次化智能，提高AI触及程度，数据压缩与高效利用"
            }
          ],
          "content": "对AI行业及芯片产业的影响"
        },
        {
          "children": [
            {
              "children": [],
              "content": "效率问题：当前AI模型与人脑在功耗、模型尺寸、学习所需数据量上存在至少三个数量级的效率差距"
            },
            {
              "children": [],
              "content": "自我提升能力：模型需具备自我学习与优化能力，解锁数据的高效利用，实现“make every token count”"
            },
            {
              "children": [],
              "content": "数据壁垒与领域知识：行业专家的经验与智慧尚未充分数据化，构成AI超越人类的瓶颈之一"
            },
            {
              "children": [],
              "content": "世界模型与强化学习：世界模型作为重要方向，结合强化学习或模型预测控制，实现自我空间内的行为优化"
            }
          ],
          "content": "AGI（通用人工智能）的展望与挑战"
        },
        {
          "children": [
            {
              "children": [],
              "content": "开源与闭源模型的竞争：开源模型如Deep Seek降低行业门槛，激发更多创新，闭源模型需保持技术领先与管理优势"
            },
            {
              "children": [],
              "content": "算法、算力与数据的协同发展：算法创新、算力优化与数据高效利用共同推动AI进步"
            },
            {
              "children": [],
              "content": "AGI实现路径的多样性：尽管存在争议，业界正探索多种路径，包括世界模型、神经符号推理、搜索算法等，以逼近类人智能"
            }
          ],
          "content": "行业生态与未来趋势"
        },
        {
          "children": [
            {
              "children": [],
              "content": "Deep Seek的出现标志着AI领域的重大进展，其开源策略与技术创新对行业生态产生深远影响"
            },
            {
              "children": [],
              "content": "AGI的实现仍面临效率、自我提升能力与数据利用等核心挑战，需跨学科基础研究与工程实践的持续探索"
            },
            {
              "children": [],
              "content": "AI行业正经历快速变革，开源与闭源模型的竞争、算力与算法的优化、数据的高效利用共同塑造着未来AI发展的格局"
            }
          ],
          "content": "结论与展望"
        }
      ],
      "content": "Deep Seek及其影响"
    }
  }
}